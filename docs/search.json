[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Scott Townsend",
    "section": "",
    "text": "About me:\nHi there! My name is Scott Townsend, and I‚Äôm a senior majoring in Data Science at Brigham Young University - Idaho. Currently, I‚Äôm working on my Senior Data Science Project, which has been an exciting and challenging journey.\nThrough this blog, I‚Äôll be sharing updates on my project as I progress, along with any insights and challenges I face along the way. I‚Äôll also showcase other data science projects I‚Äôve completed, as well as those I‚Äôm actively working on, ranging from statistical analyses to machine learning applications."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/Week_01/index.html",
    "href": "posts/Week_01/index.html",
    "title": "Week 1",
    "section": "",
    "text": "As part of my Senior Data Science Project, I‚Äôm creating a tool that automatically generates captions for images. This tool aims to help visually impaired individuals understand pictures and streamline tasks like photo tagging and social media captioning.\nThe model will be trained on the Flickr8k dataset, which contains thousands of images paired with captions. The goal is to build a model that can analyze an image and generate a relevant description using machine learning techniques.\n\n\n\nThe following deliverables are part of this project:\n\nCleaned Data: Organize the dataset by ensuring that images and captions are properly matched.\nWorking Model: A model that extracts features from images (CNN) and generates captions (LSTM/Transformer).\nPerformance Check: Evaluate how well the captions align with the images.\nVisuals: Graphs and diagrams showing the model‚Äôs performance and where it focuses in the image.\nWrite-Up: A brief explanation of the project and its potential applications.\n\n\n\n\nFirst, I load the dataset and ensure that the images and captions are properly matched:\nimport pandas as pd\nimport os\n\nimage_path = '/Users/scotttow123/Documents/BYUI/Senior_Project/Images'\ncaption_file = '/Users/scotttow123/Documents/BYUI/Senior_Project/captions.txt'\n\ntry:\n    data = pd.read_csv(caption_file)\n    print(\"Data loaded successfully\")\nexcept FileNotFoundError:\n    print(f\"Error: The file {caption_file} was not found.\")\n    data = pd.DataFrame()\n\ndata.head()\nNext, I remove any duplicates and handle missing captions:\n# Remove duplicates\nif data.duplicated().any():\n    print(f\"Found {data.duplicated().sum()} duplicate rows. Removing duplicates...\")\n    data = data.drop_duplicates()\n\n# Handle missing captions\nif data['caption'].isnull().any():\n    print(f\"Found {data['caption'].isnull().sum()} missing captions. Replacing with 'No caption'.\")\n    data['caption'] = data['caption'].fillna(\"No caption\")\n\n# Validate image paths\nvalid_image_paths = data['image'].apply(lambda x: os.path.exists(os.path.join(image_path, x)))\nif not valid_image_paths.all():\n    print(f\"Found {(~valid_image_paths).sum()} invalid image paths. Removing these rows...\")\n    data = data[valid_image_paths]\nThe code above ran successfully, indicating:\nFound 10 duplicate rows. Removing duplicates...\nThis confirmed that there were duplicates within the Flickr dataset.\nTo visualize a sample of images with their captions, I used:\ndisplay_images(data.sample(15))\nThis line of code shows a random sample of 15 images with their respective captions.\n\n\n\nRandom Sample of Images\n\n\n\n\n\nFinally, I implemented text preprocessing to clean and format the captions:\ndef text_preprocessing(data):\n    data['caption'] = data['caption'].apply(lambda x: x.lower())\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\", \"\"))\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\", \" \"))\n    data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word) &gt; 1]))\n    data['caption'] = \"startseq \" + data['caption'] + \" endseq\"\n    return data\n\ndata = text_preprocessing(data)\ncaptions = data['caption'].tolist()\ncaptions[:10]\n['startseq child in pink dress is climbing up set of stairs in an entry way endseq',\n 'startseq girl going into wooden building endseq',\n 'startseq little girl climbing into wooden playhouse endseq',\n 'startseq little girl climbing the stairs to her playhouse endseq',\n 'startseq little girl in pink dress going into wooden cabin endseq',\n 'startseq black dog and spotted dog are fighting endseq',\n 'startseq black dog and tri-colored dog playing with each other on the road endseq',\n 'startseq black dog and white dog with brown spots are staring at each other in the street endseq',\n 'startseq two dogs of different breeds looking at each other on the road endseq',\n 'startseq two dogs on pavement moving toward each other endseq']\nThis code ensures that captions are converted to lowercase, unwanted characters are removed, and sequences are formatted with start and end tokens. Above we can see a snippet of what this looks like.\n\nWith the data cleaned and preprocessed, I am now ready to move on to building and training the image captioning model. More updates to come!"
  },
  {
    "objectID": "posts/Week_01/index.html#introduction",
    "href": "posts/Week_01/index.html#introduction",
    "title": "Week 1",
    "section": "",
    "text": "As part of my Senior Data Science Project, I‚Äôm creating a tool that automatically generates captions for images. This tool aims to help visually impaired individuals understand pictures and streamline tasks like photo tagging and social media captioning.\nThe model will be trained on the Flickr8k dataset, which contains thousands of images paired with captions. The goal is to build a model that can analyze an image and generate a relevant description using machine learning techniques."
  },
  {
    "objectID": "posts/Week_01/index.html#project-deliverables",
    "href": "posts/Week_01/index.html#project-deliverables",
    "title": "Week 1",
    "section": "",
    "text": "The following deliverables are part of this project:\n\nCleaned Data: Organize the dataset by ensuring that images and captions are properly matched.\nWorking Model: A model that extracts features from images (CNN) and generates captions (LSTM/Transformer).\nPerformance Check: Evaluate how well the captions align with the images.\nVisuals: Graphs and diagrams showing the model‚Äôs performance and where it focuses in the image.\nWrite-Up: A brief explanation of the project and its potential applications."
  },
  {
    "objectID": "posts/Week_01/index.html#code-example-data-loading-and-preprocessing",
    "href": "posts/Week_01/index.html#code-example-data-loading-and-preprocessing",
    "title": "Week 1",
    "section": "",
    "text": "First, I load the dataset and ensure that the images and captions are properly matched:\nimport pandas as pd\nimport os\n\nimage_path = '/Users/scotttow123/Documents/BYUI/Senior_Project/Images'\ncaption_file = '/Users/scotttow123/Documents/BYUI/Senior_Project/captions.txt'\n\ntry:\n    data = pd.read_csv(caption_file)\n    print(\"Data loaded successfully\")\nexcept FileNotFoundError:\n    print(f\"Error: The file {caption_file} was not found.\")\n    data = pd.DataFrame()\n\ndata.head()\nNext, I remove any duplicates and handle missing captions:\n# Remove duplicates\nif data.duplicated().any():\n    print(f\"Found {data.duplicated().sum()} duplicate rows. Removing duplicates...\")\n    data = data.drop_duplicates()\n\n# Handle missing captions\nif data['caption'].isnull().any():\n    print(f\"Found {data['caption'].isnull().sum()} missing captions. Replacing with 'No caption'.\")\n    data['caption'] = data['caption'].fillna(\"No caption\")\n\n# Validate image paths\nvalid_image_paths = data['image'].apply(lambda x: os.path.exists(os.path.join(image_path, x)))\nif not valid_image_paths.all():\n    print(f\"Found {(~valid_image_paths).sum()} invalid image paths. Removing these rows...\")\n    data = data[valid_image_paths]\nThe code above ran successfully, indicating:\nFound 10 duplicate rows. Removing duplicates...\nThis confirmed that there were duplicates within the Flickr dataset.\nTo visualize a sample of images with their captions, I used:\ndisplay_images(data.sample(15))\nThis line of code shows a random sample of 15 images with their respective captions.\n\n\n\nRandom Sample of Images"
  },
  {
    "objectID": "posts/Week_03/index.html",
    "href": "posts/Week_03/index.html",
    "title": "Week 3",
    "section": "",
    "text": "Word Cloud\nBelow is a visualization of the most popular words from the caption dataset. Notably, ‚ÄúMan‚Äù and ‚ÄúWoman‚Äù emerge as the most frequently mentioned words within the captions.\n\n\n\nWord Cloud\n\n\n\n\nCaption Model Architecture\nBelow is the code for defining and compiling a neural network model designed for image captioning. The model combines image and text features, processes them through LSTM layers, and generates captions.\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Reshape, Embedding, LSTM, Dropout, add, concatenate\nfrom keras.utils import plot_model\n\n# Define input layers\ninput1 = Input(shape=(1920,))\ninput2 = Input(shape=(max_length,))\n\n# Image feature layers\nimg_features = Dense(256, activation='relu')(input1)\nimg_features_reshaped = Reshape((1, 256), input_shape=(256,))(img_features)\n\n# Text feature layers\nsentence_features = Embedding(vocab_size, 256, mask_zero=False)(input2)\nmerged = concatenate([img_features_reshaped, sentence_features], axis=1)\nsentence_features = LSTM(256)(merged)\nx = Dropout(0.5)(sentence_features)\nx = add([x, img_features])\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.5)(x)\noutput = Dense(vocab_size, activation='softmax')(x)\n\n# Compile the model\ncaption_model = Model(inputs=[input1, input2], outputs=output)\ncaption_model.compile(loss='categorical_crossentropy', optimizer='adam')\n\n\nModel Visualization\n# Plot model architecture\nplot_model(caption_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n\n# Model summary\ncaption_model.summary()\n\n\nData Generators\nThe following code sets up custom data generators for training and validation. These generators load image-caption pairs from the dataset and prepare batches for model training.\ntrain_generator = CustomDataGenerator(\n    df=train,\n    X_col='image',\n    y_col='caption',\n    batch_size=64,\n    directory=image_path,\n    tokenizer=tokenizer,\n    vocab_size=vocab_size,\n    max_length=max_length,\n    features=features\n)\n\nvalidation_generator = CustomDataGenerator(\n    df=test,\n    X_col='image',\n    y_col='caption',\n    batch_size=64,\n    directory=image_path,\n    tokenizer=tokenizer,\n    vocab_size=vocab_size,\n    max_length=max_length,\n    features=features\n)\nStay tuned for next week where we will finish the model!"
  },
  {
    "objectID": "posts/Week_03/index.html#model-architecture",
    "href": "posts/Week_03/index.html#model-architecture",
    "title": "Week 3",
    "section": "",
    "text": "The model consists of two inputs: one for the image features and another for the tokenized caption. The image features go through a dense layer, and the caption goes through an embedding layer and an LSTM. The outputs are merged, and the model learns to predict the next word in the caption.\nfrom tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add, concatenate, Reshape\n\ninput1 = Input(shape=(1920,))  # Image features from CNN\ninput2 = Input(shape=(max_length,))  # Tokenized caption\n\nimg_features = Dense(256, activation='relu')(input1)\nimg_features_reshaped = Reshape((1, 256))(img_features)\n\nsentence_features = Embedding(vocab_size, 256)(input2)\nmerged = concatenate([img_features_reshaped, sentence_features], axis=1)\nsentence_features = LSTM(256)(merged)\nx = Dropout(0.5)(sentence_features)\nx = add([x, img_features])\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.5)(x)\noutput = Dense(vocab_size, activation='softmax')(x)\n\ncaption_model = Model(inputs=[input1, input2], outputs=output)\ncaption_model.compile(loss='categorical_crossentropy', optimizer='adam')"
  },
  {
    "objectID": "posts/Week_03/index.html#training-the-model",
    "href": "posts/Week_03/index.html#training-the-model",
    "title": "Week 3",
    "section": "",
    "text": "I use the CustomDataGenerator to feed the data into the model, and the model is trained with checkpoints, early stopping, and learning rate reduction to avoid overfitting.\nhistory = caption_model.fit(\n        train_generator,\n        epochs=50,\n        validation_data=validation_generator,\n        callbacks=[checkpoint, earlystopping, learning_rate_reduction]\n)"
  },
  {
    "objectID": "posts/Week_03/index.html#final-thoughts",
    "href": "posts/Week_03/index.html#final-thoughts",
    "title": "Week 3",
    "section": "",
    "text": "Once the model is trained, I can use it to generate captions for unseen images. In the next post, I‚Äôll show how the model performs and how it generates captions for test images.\nStay tuned for more insights into how the model works and its real-world applications!"
  },
  {
    "objectID": "posts/Week_02/index.html",
    "href": "posts/Week_02/index.html",
    "title": "Week 2",
    "section": "",
    "text": "Interesting statistics:\nTotal captions: 40445 Average caption length: 11.78 words Max caption length: 38 words Min caption length: 1 words\n\n\nTokenizing Captions\nThis section handles the preparation of textual data by converting image captions into sequences of integers. It also splits the dataset into training and validation sets, ensuring that 85% of the images are used for training.\n# Tokenize captions\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(captions)\nvocab_size = len(tokenizer.word_index) + 1\nmax_length = max(len(caption.split()) for caption in captions)\n\n# Train-test split\nimages = data['image'].unique().tolist()\nnimages = len(images)\n\nsplit_index = round(0.85 * nimages)\ntrain_images = images[:split_index]\nval_images = images[split_index:]\n\ntrain = data[data['image'].isin(train_images)]\ntest = data[data['image'].isin(val_images)]\n\ntrain.reset_index(inplace=True, drop=True)\ntest.reset_index(inplace=True, drop=True)\n\ntokenizer.texts_to_sequences([captions[1]])[0]\n\n\nImage Feature Extraction\nThis section uses the DenseNet201 model to extract meaningful image features, which are saved in a dictionary for use during model training. Each image is resized, normalized, and converted into a format compatible with the model.\n# Use DenseNet201 for image feature extraction\nmodel = DenseNet201()\nfe = Model(inputs=model.input, outputs=model.layers[-2].output)\n\nimg_size = 224\nfeatures = {}\nfor image in tqdm(data['image'].unique().tolist()):\n    img = load_img(os.path.join(image_path, image), target_size=(img_size, img_size))\n    img = img_to_array(img)\n    img = img / 255.\n    img = np.expand_dims(img, axis=0)\n    feature = fe.predict(img, verbose=0)\n    features[image] = feature\n\n\nCustom Data Generator\nThis custom data generator class facilitates efficient model training by yielding batches of data, including image features and tokenized caption sequences. It handles batch creation, shuffling, and ensures compatibility with the model‚Äôs input format.\nclass CustomDataGenerator(Sequence):\n\n    def __init__(self, df, X_col, y_col, batch_size, directory, tokenizer,\n                 vocab_size, max_length, features, shuffle=True):\n\n        self.df = df.copy()\n        self.X_col = X_col\n        self.y_col = y_col\n        self.directory = directory\n        self.batch_size = batch_size\n        self.tokenizer = tokenizer\n        self.vocab_size = vocab_size\n        self.max_length = max_length\n        self.features = features\n        self.shuffle = shuffle\n        self.n = len(self.df)\n\n    def on_epoch_end(self):\n        if self.shuffle:\n            self.df = self.df.sample(frac=1).reset_index(drop=True)\n\n    def __len__(self):\n        return self.n // self.batch_size\n\n    def __getitem__(self, index):\n\n        batch = self.df.iloc[index * self.batch_size:(index + 1) * self.batch_size, :]\n        X1, X2, y = self.__get_data(batch)\n        return (X1, X2), y\n\n    def __get_data(self, batch):\n\n        X1, X2, y = list(), list(), list()\n\n        images = batch[self.X_col].tolist()\n\n        for image in images:\n            feature = self.features[image][0]\n\n            captions = batch.loc[batch[self.X_col] == image, self.y_col].tolist()\n            for caption in captions:\n                seq = self.tokenizer.texts_to_sequences([caption])[0]\n\n                for i in range(1, len(seq)):\n                    in_seq, out_seq = seq[:i], seq[i]\n                    in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]\n                    out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]\n                    X1.append(feature)\n                    X2.append(in_seq)\n                    y.append(out_seq)\n\n        X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n\n        return X1, X2, y\nNext week we will look at the model creation and the image and text feature layers!"
  },
  {
    "objectID": "posts/Week_02/index.html#image-preprocessing",
    "href": "posts/Week_02/index.html#image-preprocessing",
    "title": "Week 2",
    "section": "",
    "text": "For each image, I resize it and normalize the pixel values to a range between 0 and 1:\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n\ndef readImage(path, img_size=224):\n    img = load_img(path, color_mode='rgb', target_size=(img_size, img_size))\n    img = img_to_array(img)\n    img = img / 255.  # Normalize\n    return img\n\ndef text_preprocessing(data):\n    data['caption'] = data['caption'].apply(lambda x: x.lower())\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\", \"\"))\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\", \" \"))\n    data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word) &gt; 1]))\n    data['caption'] = \"startseq \" + data['caption'] + \" endseq\"\n    return data\n\ndata = text_preprocessing(data)\ncaptions = data['caption'].tolist()\ncaptions[:10]\nNext, I will tokenize the captions, splitting them into words and assigning each word a unique integer.\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(captions)\nvocab_size = len(tokenizer.word_index) + 1  # Include padding token\nmax_length = max(len(caption.split()) for caption in captions)\n\n# Split into train and test\nimages = data['image'].unique().tolist()\nsplit_index = round(0.85 * len(images))\ntrain_images = images[:split_index]\nval_images = images[split_index:]\n\ntrain = data[data['image'].isin(train_images)]\ntest = data[data['image'].isin(val_images)]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Senior Project Blog",
    "section": "",
    "text": "Week 3\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 11, 2025\n\n\nScott Townsend\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 4, 2025\n\n\nScott Townsend\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 1\n\n\n\n\n\n\nnews\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 27, 2025\n\n\nScott Townsend\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Streaming Services Analysis\n\n\n\n\n\n\nPython\n\n\nDataviz\n\n\n\n\n\n\n\n\n\nMar 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCrime Statistics Analysis\n\n\n\n\n\n\nPython\n\n\nDataviz\n\n\n\n\n\n\n\n\n\nFeb 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nStudent Performance ML\n\n\n\n\n\n\nPython\n\n\nML\n\n\n\n\n\n\n\n\n\nDec 16, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html#projects",
    "href": "projects.html#projects",
    "title": "üìö Projects",
    "section": "",
    "text": "A collection of my personal projects showcasing my work in data analysis, statistical modeling, and machine learning.\n\n\n\nStreaming Services Data Analysis (January 2025 - Present)\nAnalyzed Netflix, Hulu, Disney+, and Amazon content libraries to uncover trends in movie durations, genres, and ratings.\n\nUtilized Polars and Pandas for efficient data processing.\n\nVisualized insights with Seaborn, Matplotlib, and Plotly, exploring genre popularity, rating distributions, and country-wise content production.\n\nBuilt interactive dashboards and statistical summaries to inform data-driven decisions.\n\nImage Captioning Tool for Visually Impaired Users (December 2024 - Present)\nDeveloped a deep learning model to generate descriptive captions for images, aiding visually impaired individuals and automating social media captioning.\n\nImplemented a CNN (VGG16) for image feature extraction and an LSTM/Transformer for caption generation.\n\nVisualized model attention areas using heatmaps and presented insights through data visualizations.\n\nCrime Analysis\nExploring crime trends through data analysis and visualization.\nStudent Performance Analysis\nAnalyzing factors affecting student academic performance.\nStudent Performance Predictive ML Model\nBuilding a predictive model to forecast student success based on historical data.\nPopulation Growth Analysis\nInvestigating population growth trends using statistical methods.\n\nüí° Feel free to explore my projects for detailed analysis and visualization."
  },
  {
    "objectID": "projects.html#my-current-projects",
    "href": "projects.html#my-current-projects",
    "title": "üìö Projects",
    "section": "",
    "text": "A collection of my personal projects showcasing my work in data analysis, statistical modeling, and machine learning.\n\n\n\nStreaming Services Data Analysis (January 2025 - Present)\nAnalyzed Netflix, Hulu, Disney+, and Amazon content libraries to uncover trends in movie durations, genres, and ratings.\n\nUtilized Polars and Pandas for efficient data processing.\n\nVisualized insights with Seaborn, Matplotlib, and Plotly, exploring genre popularity, rating distributions, and country-wise content production.\n\nBuilt interactive dashboards and statistical summaries to inform data-driven decisions.\n\nImage Captioning Tool for Visually Impaired Users (December 2024 - Present)\nDeveloped a deep learning model to generate descriptive captions for images, aiding visually impaired individuals and automating social media captioning.\n\nImplemented a CNN (VGG16) for image feature extraction and an LSTM/Transformer for caption generation.\n\nVisualized model attention areas using heatmaps and presented insights through data visualizations.\n\nCrime Analysis\nExploring crime trends through data analysis and visualization. Developed interactive visualizations in Python (Matplotlib, Plotly) to highlight trends in the demographics of offenders and victims.\n\nApplied data wrangling techniques to identify patterns and trends, offering actionable insights into public safety strategies.\nAnalyzed offender-victim relationships to uncover societal trends influencing crime rates.\n\nStudent Performance Analysis\nAnalyzing factors affecting student academic performance.\nStudent Performance Predictive ML Model\nBuilding a predictive model to forecast student success based on historical data.\nPopulation Growth Analysis\nInvestigating population growth trends using statistical methods.\n\nüí° Feel free to explore my projects for detailed analysis and visualization."
  },
  {
    "objectID": "projects/Crime_Analysis/index.html",
    "href": "projects/Crime_Analysis/index.html",
    "title": "Crime Statistics Analysis",
    "section": "",
    "text": "Crime Statstics Analysis"
  },
  {
    "objectID": "projects/Crime_Analysis/index.html#analysis-overview",
    "href": "projects/Crime_Analysis/index.html#analysis-overview",
    "title": "Criminal Incident Analysis",
    "section": "",
    "text": "The analysis is structured into several key sections, each exploring a different aspect of criminal incidents. Below are the main insights drawn from the data:\n\n\nThis visualization provides an overview of the frequency of criminal incidents based on their location. Understanding where crimes occur helps in targeting both public and private safety initiatives.\n\n\n\n\nResidential areas are the most common settings for criminal incidents, followed closely by public locations such as streets and sidewalks. These findings underscore the need for improved safety measures in both private homes and public spaces.\n\n\n\n\n\nThe following charts show the age distributions of both offenders and victims, offering insights into the typical age groups involved in these incidents.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOffenders: The most common age group among offenders is 20‚Äì29, followed by 30‚Äì39. This indicates that younger adults are more likely to be involved in criminal activities.\nVictims: Victimization also peaks in the 20‚Äì29 age group, suggesting a strong overlap between the ages of offenders and victims. This may reflect social or environmental factors influencing both offending and victimization patterns.\n\n\n\n\n\nThe charts below compare the ethnicities of offenders and victims to reveal potential patterns of racial disparities in criminal incidents.\n\n\n\nEthnicties of the Offenders\n\n\n\n\n\n\n\n\n\n\nThe majority of both offenders and victims are categorized as non-Hispanic and non-Latino. This suggests that ethnic groups outside of the Hispanic/Latino community are more frequently involved in criminal incidents. Further analysis is needed to explore the sociocultural and systemic factors behind these patterns.\n\n\n\n\n\nThis analysis explores the racial distribution of offenders and victims, investigating whether race-related disparities are present.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOffenders: Both White and African American individuals are most commonly represented among offenders, highlighting possible broader societal and economic influences.\nVictims: White individuals appear to be more frequently victimized, suggesting potential interactions between race, geographic location, and social dynamics. This warrants further investigation into how systemic factors contribute to these disparities.\n\n\n\n\n\nThis chart categorizes the types of offenses involved in the reported incidents, giving us a detailed breakdown of criminal activity.\n\n\n\n\n\n\n\n\nProperty destruction is the most prevalent offense, followed by simple assaults, breaking and entering, and drug-related crimes. This breakdown provides a clearer picture of the types of criminal activities most commonly occurring in these incidents.\n\n\n\n\n\nThe following chart provides an overview of the types of weapons used during criminal incidents, offering valuable insights into the role of weaponry in violent crime.\n\n\n\n\n\n\n\n\nPersonal weapons (hands, fists, etc.) are the most frequently used in incidents, followed by firearms and knives. The prominence of physical altercations emphasizes the need for preventive measures around conflict de-escalation, while the prevalence of firearms and knives highlights the importance of effective weapon control.\n\n\n\n\n\nThis chart examines the relationship dynamics between offenders and victims, uncovering patterns in how these relationships might influence criminal behavior.\n\n\n\n\n\n\n\n\nThe most common relationship between offenders and victims is that of strangers, where no prior connection exists between the parties. Other significant relationships include boyfriends/girlfriends, friends, spouses, and acquaintances. This indicates a range of scenarios, from random violence to incidents stemming from interpersonal conflicts."
  },
  {
    "objectID": "projects/Crime_Analysis/index.html#conclusion",
    "href": "projects/Crime_Analysis/index.html#conclusion",
    "title": "Criminal Incident Analysis",
    "section": "",
    "text": "This analysis uncovers several critical patterns in criminal activity related to gun violence. Key findings include:\n\nYounger individuals, particularly in their 20s and 30s, are more likely to be involved both as offenders and victims.\nThe most common offenses involve property destruction, assaults, and drug-related crimes.\nBoth White and African American individuals are most commonly represented among offenders, while White individuals are more frequently victimized.\nPersonal weapons and firearms are the most common weapons used in violent incidents.\n\nThese insights are crucial for understanding the social dynamics behind gun violence. By further exploring these trends and integrating additional datasets, law enforcement agencies and policymakers can make more informed decisions about crime prevention, resource allocation, and public safety initiatives."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Senior Project Blog",
    "section": "",
    "text": "Ridgelines in R\n\n\n\n\n\n\nR\n\n\nDataviz\n\n\n\n\n\n\n\n\n\nFeb 23, 2025\n\n\nScott Townsend\n\n\n\n\n\n\n\n\n\n\n\n\nRidgelines in Python\n\n\n\n\n\n\nPython\n\n\nDataviz\n\n\n\n\n\n\n\n\n\nFeb 23, 2025\n\n\nScott Townsend\n\n\n\n\n\n\n\n\n\n\n\n\nResume\n\n\n\n\n\n\nQuarto\n\n\nLaTeX\n\n\n\n\n\n\n\n\n\nFeb 23, 2025\n\n\nScott Townsend\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial Charts in R\n\n\n\n\n\n\nR\n\n\nDataviz\n\n\n\n\n\n\n\n\n\nFeb 23, 2025\n\n\nScott Townsend\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 11, 2025\n\n\nScott Townsend\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 4, 2025\n\n\nScott Townsend\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 1\n\n\n\n\n\n\nnews\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 27, 2025\n\n\nScott Townsend\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Week_01/index.html#text-preprocessing",
    "href": "posts/Week_01/index.html#text-preprocessing",
    "title": "Week 1",
    "section": "",
    "text": "Finally, I implemented text preprocessing to clean and format the captions:\ndef text_preprocessing(data):\n    data['caption'] = data['caption'].apply(lambda x: x.lower())\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\", \"\"))\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\", \" \"))\n    data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word) &gt; 1]))\n    data['caption'] = \"startseq \" + data['caption'] + \" endseq\"\n    return data\n\ndata = text_preprocessing(data)\ncaptions = data['caption'].tolist()\ncaptions[:10]\n['startseq child in pink dress is climbing up set of stairs in an entry way endseq',\n 'startseq girl going into wooden building endseq',\n 'startseq little girl climbing into wooden playhouse endseq',\n 'startseq little girl climbing the stairs to her playhouse endseq',\n 'startseq little girl in pink dress going into wooden cabin endseq',\n 'startseq black dog and spotted dog are fighting endseq',\n 'startseq black dog and tri-colored dog playing with each other on the road endseq',\n 'startseq black dog and white dog with brown spots are staring at each other in the street endseq',\n 'startseq two dogs of different breeds looking at each other on the road endseq',\n 'startseq two dogs on pavement moving toward each other endseq']\nThis code ensures that captions are converted to lowercase, unwanted characters are removed, and sequences are formatted with start and end tokens. Above we can see a snippet of what this looks like.\n\nWith the data cleaned and preprocessed, I am now ready to move on to building and training the image captioning model. More updates to come!"
  },
  {
    "objectID": "projects/Streaming_Services/index.html",
    "href": "projects/Streaming_Services/index.html",
    "title": "Streaming Services Analysis",
    "section": "",
    "text": "You can slide the charts around to focus on different aspects of it or see different parts."
  },
  {
    "objectID": "projects/Streaming_Services/index.html#analysis-overview",
    "href": "projects/Streaming_Services/index.html#analysis-overview",
    "title": "Streaming Services Analysis",
    "section": "Analysis Overview:",
    "text": "Analysis Overview:\nThis analysis will dive into data from four leading streaming platforms‚ÄîNetflix, Hulu, Amazon Prime, and Disney+‚Äîto explore trends, viewership patterns, and key performance metrics. By examining each service‚Äôs unique offerings and subscriber behaviors, we aim to uncover insights that highlight their market positions and user preferences in the ever-evolving digital entertainment landscape.\n\nDistribution of Movie/Show Durations by Streaming Service\n\nimport pandas as pd\nimport polars as pl\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# --- Load Data ---\ndisney = pl.read_csv(\"/Users/scotttow123/Documents/Streaming_Services/Data/disney_plus_titles.csv\")\nhulu = pl.read_csv(\"/Users/scotttow123/Documents/Streaming_Services/Data/hulu_titles.csv\")\nnetflix = pl.read_csv(\"/Users/scotttow123/Documents/Streaming_Services/Data/netflix_titles.csv\")\nprime = pd.read_csv(\"/Users/scotttow123/Documents/Streaming_Services/Data/amazon_prime_titles.csv\")\n\ndisney = disney.with_columns(pl.lit(\"Disney+\").alias(\"platform\"))\nhulu = hulu.with_columns(pl.lit(\"Hulu\").alias(\"platform\"))\nnetflix = netflix.with_columns(pl.lit(\"Netflix\").alias(\"platform\"))\n\ndisney = disney.to_pandas()\nhulu = hulu.to_pandas()\nnetflix = netflix.to_pandas()\n\nprime[\"platform\"] = \"Amazon Prime\"\n\ndata = pd.concat([disney, hulu, netflix, prime], ignore_index=True)\n\ndef convert_duration(duration):\n    if pd.isna(duration):\n        return None\n    duration = str(duration).lower()\n    if \"min\" in duration:\n        return int(duration.replace(\" min\", \"\"))\n    elif \"h\" in duration:\n        parts = duration.split(\" \")\n        hours = int(parts[0].replace(\"h\", \"\")) * 60\n        minutes = int(parts[1].replace(\"min\", \"\")) if len(parts) &gt; 1 else 0\n        return hours + minutes\n    return None\n\ndata[\"duration_minutes\"] = data[\"duration\"].apply(convert_duration)\n\ndata[\"date_added\"] = pd.to_datetime(data[\"date_added\"], errors=\"coerce\")\ndata[\"year_added\"] = data[\"date_added\"].dt.year\n\nplt.figure(figsize=(8, 6))\nsns.violinplot(data=data, x=\"platform\", y=\"duration_minutes\", palette=\"Set2\")\nplt.title(\"Distribution of Movie/Show Durations by Streaming Service\")\nplt.xlabel(\"Streaming Service\")\nplt.ylabel(\"Duration (Minutes)\")\nplt.xticks(rotation=45)\nplt.show()\n\n/var/folders/zs/zycd14f96ks5gpx0bsb97xtr0000gn/T/ipykernel_87051/3666423932.py:43: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\nThe violin plot reveals a striking contrast in content duration across platforms. Amazon Prime stands out with the broadest range, extending to exceptionally lengthy content (up to 600 minutes), suggesting a focus on longer films and series. In contrast, Disney+ exhibits the shortest content range, implying a preference for more concise viewing experiences. Netflix and Hulu fall somewhere in between, with Netflix showing a slightly wider spread than Hulu.\n\n\nNumber of Titles Added Over Time\n\nplt.figure(figsize=(18, 6))\ndf_yearly = data.groupby([\"year_added\", \"platform\"]).size().reset_index(name=\"count\")\nsns.lineplot(data=df_yearly, x=\"year_added\", y=\"count\", hue=\"platform\", marker=\"o\")\nplt.title(\"Number of Titles Added Over Time\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Number of Titles\")\nplt.legend(title=\"Streaming Service\")\nplt.show()\n\n\n\n\n\n\n\n\nThis chart highlights the aggressive content acquisition strategy employed by Netflix. Starting around 2016, Netflix embarked on a dramatic expansion of its library, consistently adding new titles at a pace that far surpassed its competitors. While other platforms also increased their offerings, their growth trajectories were notably less steep, underscoring Netflix‚Äôs commitment to content dominance.\n\n\nDistribution of Movie Ratings Across Platforms\n\nplt.figure(figsize=(30, 6))\nsns.histplot(data=data, x=\"rating\", hue=\"platform\", multiple=\"stack\", palette=\"Set1\", shrink=0.8)\nplt.title(\"Distribution of Movie Ratings Across Platforms\")\nplt.xlabel(\"Rating\")\nplt.ylabel(\"Count of Titles\")\nplt.xticks(rotation=45)\nplt.legend(title=\"Streaming Service\")\nplt.show()\n\n/var/folders/zs/zycd14f96ks5gpx0bsb97xtr0000gn/T/ipykernel_87051/2575985119.py:7: UserWarning:\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\n\n\n\n\nThe distribution of movie ratings reveals distinct content preferences across platforms. Amazon Prime caters to a more mature audience with a significant concentration of TV-MA rated content. In contrast, Disney+ likely focuses on family-friendly viewing, as suggested by its lower proportion of mature ratings (though specific data on G and PG ratings is not available here). Netflix and Hulu demonstrate a more balanced approach, offering a mix of ratings to appeal to a broader audience."
  },
  {
    "objectID": "projects/Streaming_Services/index.html#conclusion",
    "href": "projects/Streaming_Services/index.html#conclusion",
    "title": "Streaming Services Analysis",
    "section": "Conclusion:",
    "text": "Conclusion:\nThis analysis has illuminated key differences in content strategies across the four streaming platforms. Each exhibits distinct trends in content offerings, duration, and release timing, catering to diverse viewer preferences. Amazon Prime, with its extensive library and broad range of content durations, including a substantial offering of mature titles, appears to prioritize attracting a diverse and mature audience. Netflix, while showing signs of slowing growth in recent years, still boasts a massive library and a history of aggressive content acquisition, suggesting a focus on breadth and variety."
  },
  {
    "objectID": "posts/Ridgelines/index.html",
    "href": "posts/Ridgelines/index.html",
    "title": "Ridgelines in Python",
    "section": "",
    "text": "In the world of data visualization, understanding the distribution of data across different categories is crucial. Ridgeline plots, also known as joyplots, offer an elegant and effective way to visualize these distributions. This blog post will guide you through creating ridgeline plots in Python using seaborn and matplotlib.\n\n\nRidgeline plots display the distribution of a numerical variable across multiple categories by plotting density estimates (or histograms) that are stacked vertically and slightly overlapped. This creates a ‚Äúridgeline‚Äù effect, making it easy to compare the distributions of different groups.\nThese plots are particularly useful for:\n\nComparing distributions: Quickly identifying differences in shape, spread, and central tendency across categories.\nIdentifying patterns: Spotting trends and shifts in data that might be obscured in other visualization types.\nEnhancing visual appeal: Creating engaging and informative graphics.\n\n\n\n\nLet‚Äôs illustrate how to create ridgeline plots using a simulated dataset of monthly temperature distributions. We‚Äôll utilize the seaborn and matplotlib libraries, which are essential for this task.\nFirst, ensure you have the necessary libraries installed:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(123)\n\nmonths = ['January', 'February', 'March', 'April', 'May', 'June', \n          'July', 'August', 'September', 'October', 'November', 'December']\nn = 100\n\ndata = pd.DataFrame({\n    'month': np.repeat(months, n),\n    'temperature': np.concatenate([\n        np.random.normal(20, 5, n),    # January\n        np.random.normal(25, 6, n),    # February\n        np.random.normal(30, 7, n),    # March\n        np.random.normal(40, 8, n),    # April\n        np.random.normal(50, 9, n),    # May\n        np.random.normal(60, 10, n),   # June\n        np.random.normal(65, 10, n),   # July\n        np.random.normal(62, 9, n),    # August\n        np.random.normal(55, 8, n),    # September\n        np.random.normal(45, 7, n),    # October\n        np.random.normal(35, 6, n),    # November\n        np.random.normal(28, 5, n)     # December\n    ])\n})\n\ndata['month'] = pd.Categorical(data['month'], categories=months, ordered=True)\n\nplt.figure(figsize=(10, 8))\nsns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n\ng = sns.FacetGrid(data, row=\"month\", hue=\"month\", aspect=15, height=0.5, palette=\"plasma\", row_order=months[::-1])  \n\ng.map_dataframe(sns.kdeplot, \"temperature\", fill=True, alpha=1)\n\ndef add_median(data, **kwargs):\n    median = data['temperature'].median()\n    plt.axvline(median, color='black', linestyle='--', linewidth=1)\n\ng.map_dataframe(add_median)\n\ndef label(data, color, label): \n    ax = plt.gca()\n    ax.text(0, 0.2, label, fontweight=\"bold\", color=color, ha=\"left\", va=\"center\", transform=ax.transAxes)\n\ng.map_dataframe(label)\n\ng.set_titles(\"\")\ng.set(yticks=[])\ng.despine(left=True)\ng.fig.subplots_adjust(hspace=-0.25)\ng.set_axis_labels(\"Average temperature (F)\", \"\")\ng.fig.suptitle(\"Monthly Temperature Distribution\", fontsize=16)\n\nplt.show()\nThis python code should create the following chart:"
  },
  {
    "objectID": "projects/Streaming_Services/index.html#content-volume-and-trends",
    "href": "projects/Streaming_Services/index.html#content-volume-and-trends",
    "title": "Streaming Services Analysis",
    "section": "",
    "text": "This visualization illustrates the proportion of the different platforms and their count of releases throughout the years. It appears that Amazon Prime has the highest count of releases for now, with Netflix close behind.\n\n\n\n\nThis line graph tracks the growth in content offerings for each platform over time. We can see here that among the platforms Amazon Prime again has the highest release count through the years. Although, Disney+ seems to have remained steady with their releases over time.\n\n\n\n\nThis visualization highlights the release patterns of movies and TV shows over time across all platforms. This graphic displays how much more prominent TV shows has become over the years with their value increasing more than Movies have."
  },
  {
    "objectID": "projects/Streaming_Services/index.html#content-duration-analysis",
    "href": "projects/Streaming_Services/index.html#content-duration-analysis",
    "title": "Streaming Services Analysis",
    "section": "",
    "text": "This plot provides a comparative analysis of the average content duration on each platform. We have Movie and TV shows being displayed, and it appears that Amazon Prime has the longest duration in both movies and tv shows, while Disney+ has the shortest.\n\n\n\n\nThis is another boxplot that examines the variation in content duration across all platforms, showing again how Amazon Prime dominates in terms of content duration with Netflix coming in second place.\n\n\n\n\nHere is one final boxplot offering a unique point of view displaying the distribution of ratings across the different platforms. It‚Äôs interesting seeing the different shapes/sizes of each box plot. Amazon Prime seems to have the widest distribution of ratings compared to the rest.\n\n\n\n\nThis chart explores the relationship between content duration and release year. Something interesting worth noting is how dominant Amazon Prime seems to be in this graphic, which makes sense in terms of our other graphics.\n\n\n\n\nHere is one additional scatter plot, showing us a more unique view of the relase year vs duration, with the rating being colored. The blue and pink seems to stand out the most among all the other colors, indicating that TV-14 and TV-Y are the most popular."
  },
  {
    "objectID": "posts/Resume/index.html#implementation",
    "href": "posts/Resume/index.html#implementation",
    "title": "Resume",
    "section": "Implementation",
    "text": "Implementation\nThe development of this resume is somewhat complex as it implements LaTeX in Quarto to beautifully format the perfect Resume. I have been using this in VS Code, where you can easily render the work once you format or adjust the LaTeX and markdown syntax to your liking.\nAs I have been editing my resume in VS Code using this tool, I have personally found the ability to edit the document while simultaneously viewing the changes side by side to be extremely helpful. Below is a snippet of what it looks like for me as I am editing.\n\nPlease note, for now this is only able to render the LaTeX resume document into a pdf. I plan to mess around and adjust some things in the near future so you can export it as an html if thats the sort of thing that you need as well.\nAdditionally, some cool features of the exported pdf include clickable links (all text in blue are actual links) that the reader can visit, so populate those spaces with urls to your work to make yourself more discoverable.\nThe finished version can be seen below:\n\nYou can fork or clone the repository using the below link if you want to implement this somehow! All credit goes to Alex Bass or acbass49 on GitHub. I have only made minor adjustments and changes to suit my needs. I only want to make this more discoverable by others struggling to find templates or formats that they enjoy or are satisfied with.\nCopy and paste this link into an IDE terminal of your choice (VS Code preferred) and update the files so it is personalized to you.\n# Clone this repository and adjust it to your liking! \ngit clone https://github.com/acbass49/CV_Quarto.git"
  },
  {
    "objectID": "posts/Resume/index.html#introduction",
    "href": "posts/Resume/index.html#introduction",
    "title": "Resume",
    "section": "Introduction",
    "text": "Introduction\nI remember making my first resume when I was in High School about 8 years as a Word Document. I hated how difficult it was to format it to my liking. I struggled with finding good templates and constantly shifted the wording, placement, and overall style of my Resume. Recently, in my Senior Data Science Project course my professor admonished prospective Data Scientists for still using Word, when tools such as Quarto existed. This got me interested in creating a Resume that exported a Mark down document into a pdf. I was directed to several resources, where I ultimately landed on the repository below that you can customized to your liking by: Alex Bass or acbass49 on GitHub"
  },
  {
    "objectID": "posts/Ridgelines2/index.html",
    "href": "posts/Ridgelines2/index.html",
    "title": "Ridgelines in R",
    "section": "",
    "text": "In the realm of data visualization, the quest for insightful and aesthetically pleasing representations is ongoing. Enter ridgeline plots, a powerful tool for displaying distributions across multiple categories. This blog post will explore what ridgeline plots are, how to create them in R using the ggplot2 and ggridges packages, and why they can be a valuable addition to your data analysis toolkit.\n\n\nRidgeline plots, also known as joyplots, are a type of visualization that displays the distribution of a numerical variable for several groups. They achieve this by plotting density estimates (or histograms) for each group, stacked vertically and slightly overlapping. This overlapping effect creates a ‚Äúridgeline‚Äù appearance, hence the name.\nThese plots are particularly effective for:\n\nComparing distributions: Quickly observe how distributions vary across different categories.\nIdentifying patterns: Spot trends and shifts in data that might be obscured in other visualization types.\nEnhancing visual appeal: Create visually engaging and informative graphics.\n\n\n\n\nTo illustrate how to create ridgeline plots, let‚Äôs use a simulated dataset of monthly temperature distributions. We‚Äôll utilize the ggplot2 and ggridges packages, which are essential for this task.\nlibrary(ggplot2)\nlibrary(ggridges)\n\nset.seed(123)\n\nmonths &lt;- month.name \nn &lt;- 100 \n\ndata &lt;- data.frame(\n  month = rep(months, each = n),\n  temperature = c(\n    rnorm(n, mean = 20, sd = 5),    # January\n    rnorm(n, mean = 25, sd = 6),    # February\n    rnorm(n, mean = 30, sd = 7),    # March\n    rnorm(n, mean = 40, sd = 8),    # April\n    rnorm(n, mean = 50, sd = 9),    # May\n    rnorm(n, mean = 60, sd = 10),   # June\n    rnorm(n, mean = 65, sd = 10),   # July\n    rnorm(n, mean = 62, sd = 9),    # August\n    rnorm(n, mean = 55, sd = 8),    # September\n    rnorm(n, mean = 45, sd = 7),    # October\n    rnorm(n, mean = 35, sd = 6),    # November\n    rnorm(n, mean = 28, sd = 5)     # December\n  )\n)\n\ndata$month &lt;- factor(data$month, levels = months)\n\nggplot(data, aes(x = temperature, y = month, fill = month)) +\n  geom_density_ridges(\n    scale = 0.95, \n    rel_min_height = 0.01, \n    quantile_lines = TRUE, \n    quantile_fun = function(x, ...) median(x)\n  ) +\n  scale_fill_viridis_d(option = \"plasma\", direction = -1) + \n  labs(\n    x = \"Average temperature (F)\",\n    y = NULL, \n    title = \"Monthly Temperature Distribution\"\n  ) +\n  theme_ridges(grid = FALSE) + \n  theme(legend.position = \"none\") \nThe above code in R will create the following image:"
  },
  {
    "objectID": "posts/Ridgelines2/index.html#what-are-ridgeline-plots",
    "href": "posts/Ridgelines2/index.html#what-are-ridgeline-plots",
    "title": "Ridgelines in R",
    "section": "",
    "text": "Ridgeline plots, also known as joyplots, are a type of visualization that displays the distribution of a numerical variable for several groups. They achieve this by plotting density estimates (or histograms) for each group, stacked vertically and slightly overlapping. This overlapping effect creates a ‚Äúridgeline‚Äù appearance, hence the name.\nThese plots are particularly effective for:\n\nComparing distributions: Quickly observe how distributions vary across different categories.\nIdentifying patterns: Spot trends and shifts in data that might be obscured in other visualization types.\nEnhancing visual appeal: Create visually engaging and informative graphics."
  },
  {
    "objectID": "posts/Ridgelines2/index.html#creating-ridgeline-plots-in-r",
    "href": "posts/Ridgelines2/index.html#creating-ridgeline-plots-in-r",
    "title": "Ridgelines in R",
    "section": "",
    "text": "To illustrate how to create ridgeline plots, let‚Äôs use a simulated dataset of monthly temperature distributions. We‚Äôll utilize the ggplot2 and ggridges packages, which are essential for this task.\nlibrary(ggplot2)\nlibrary(ggridges)\n\nset.seed(123)\n\nmonths &lt;- month.name \nn &lt;- 100 \n\ndata &lt;- data.frame(\n  month = rep(months, each = n),\n  temperature = c(\n    rnorm(n, mean = 20, sd = 5),    # January\n    rnorm(n, mean = 25, sd = 6),    # February\n    rnorm(n, mean = 30, sd = 7),    # March\n    rnorm(n, mean = 40, sd = 8),    # April\n    rnorm(n, mean = 50, sd = 9),    # May\n    rnorm(n, mean = 60, sd = 10),   # June\n    rnorm(n, mean = 65, sd = 10),   # July\n    rnorm(n, mean = 62, sd = 9),    # August\n    rnorm(n, mean = 55, sd = 8),    # September\n    rnorm(n, mean = 45, sd = 7),    # October\n    rnorm(n, mean = 35, sd = 6),    # November\n    rnorm(n, mean = 28, sd = 5)     # December\n  )\n)\n\ndata$month &lt;- factor(data$month, levels = months)\n\nggplot(data, aes(x = temperature, y = month, fill = month)) +\n  geom_density_ridges(\n    scale = 0.95, \n    rel_min_height = 0.01, \n    quantile_lines = TRUE, \n    quantile_fun = function(x, ...) median(x)\n  ) +\n  scale_fill_viridis_d(option = \"plasma\", direction = -1) + \n  labs(\n    x = \"Average temperature (F)\",\n    y = NULL, \n    title = \"Monthly Temperature Distribution\"\n  ) +\n  theme_ridges(grid = FALSE) + \n  theme(legend.position = \"none\") \nThe above code in R will create the following image:"
  },
  {
    "objectID": "posts/Ridgelines/index.html#what-are-ridgeline-plots",
    "href": "posts/Ridgelines/index.html#what-are-ridgeline-plots",
    "title": "Ridgelines in Python",
    "section": "",
    "text": "Ridgeline plots display the distribution of a numerical variable across multiple categories by plotting density estimates (or histograms) that are stacked vertically and slightly overlapped. This creates a ‚Äúridgeline‚Äù effect, making it easy to compare the distributions of different groups.\nThese plots are particularly useful for:\n\nComparing distributions: Quickly identifying differences in shape, spread, and central tendency across categories.\nIdentifying patterns: Spotting trends and shifts in data that might be obscured in other visualization types.\nEnhancing visual appeal: Creating engaging and informative graphics."
  },
  {
    "objectID": "posts/Ridgelines/index.html#creating-ridgeline-plots-in-python",
    "href": "posts/Ridgelines/index.html#creating-ridgeline-plots-in-python",
    "title": "Ridgelines in Python",
    "section": "",
    "text": "Let‚Äôs illustrate how to create ridgeline plots using a simulated dataset of monthly temperature distributions. We‚Äôll utilize the seaborn and matplotlib libraries, which are essential for this task.\nFirst, ensure you have the necessary libraries installed:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(123)\n\nmonths = ['January', 'February', 'March', 'April', 'May', 'June', \n          'July', 'August', 'September', 'October', 'November', 'December']\nn = 100\n\ndata = pd.DataFrame({\n    'month': np.repeat(months, n),\n    'temperature': np.concatenate([\n        np.random.normal(20, 5, n),    # January\n        np.random.normal(25, 6, n),    # February\n        np.random.normal(30, 7, n),    # March\n        np.random.normal(40, 8, n),    # April\n        np.random.normal(50, 9, n),    # May\n        np.random.normal(60, 10, n),   # June\n        np.random.normal(65, 10, n),   # July\n        np.random.normal(62, 9, n),    # August\n        np.random.normal(55, 8, n),    # September\n        np.random.normal(45, 7, n),    # October\n        np.random.normal(35, 6, n),    # November\n        np.random.normal(28, 5, n)     # December\n    ])\n})\n\ndata['month'] = pd.Categorical(data['month'], categories=months, ordered=True)\n\nplt.figure(figsize=(10, 8))\nsns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n\ng = sns.FacetGrid(data, row=\"month\", hue=\"month\", aspect=15, height=0.5, palette=\"plasma\", row_order=months[::-1])  \n\ng.map_dataframe(sns.kdeplot, \"temperature\", fill=True, alpha=1)\n\ndef add_median(data, **kwargs):\n    median = data['temperature'].median()\n    plt.axvline(median, color='black', linestyle='--', linewidth=1)\n\ng.map_dataframe(add_median)\n\ndef label(data, color, label): \n    ax = plt.gca()\n    ax.text(0, 0.2, label, fontweight=\"bold\", color=color, ha=\"left\", va=\"center\", transform=ax.transAxes)\n\ng.map_dataframe(label)\n\ng.set_titles(\"\")\ng.set(yticks=[])\ng.despine(left=True)\ng.fig.subplots_adjust(hspace=-0.25)\ng.set_axis_labels(\"Average temperature (F)\", \"\")\ng.fig.suptitle(\"Monthly Temperature Distribution\", fontsize=16)\n\nplt.show()\nThis python code should create the following chart:"
  },
  {
    "objectID": "posts/Spatial/index.html",
    "href": "posts/Spatial/index.html",
    "title": "Spatial Charts in R",
    "section": "",
    "text": "Spatial charts are a powerful way to visualize geographic data, allowing for insightful analysis of regional trends, patterns, and distributions. In R, spatial charts can be created using libraries like ggplot2, sf, and USAboundaries, among others.\nHere is one I created in R!\n\n\n\nGeographical Insights: Easily identify trends that vary across regions.\nEnhanced Data Interpretation: Spot high-density areas, hot spots, or outliers.\nInteractive Potential: Use tools like plotly to make maps dynamic and exploratory.\n\n\n\nlibrary(USAboundaries)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(plotly)\n\nwest_virginia_counties &lt;- us_counties() %&gt;%\n  select(geoid, name, state_abbr, geometry) %&gt;%  \n  filter(state_abbr == \"WV\") %&gt;%\n  mutate(population = runif(n(), 5000, 1000000)) %&gt;%  \n  mutate(pop_density = population / as.numeric(st_area(geometry))) \n\nwv_map &lt;- ggplot() +\n  geom_sf(data = virginia_counties, aes(fill = log1p(pop_density), text = name), color = \"white\") +\n  scale_fill_viridis_c(option = \"magma\", name = \"Pop Density (log)\") +\n  labs(\n    title = \"Population Density by County in West Virginia\",\n    subtitle = \"Log-transformed for better visualization\",\n    caption = \"Source: USAboundaries R package\"\n  ) +\n  theme_minimal()\n\nggplotly(wv_map, tooltip = \"text\")"
  },
  {
    "objectID": "posts/Spatial/index.html#why-use-spatial-charts",
    "href": "posts/Spatial/index.html#why-use-spatial-charts",
    "title": "Spatial Charts in R",
    "section": "",
    "text": "Geographical Insights: Easily identify trends that vary across regions.\nEnhanced Data Interpretation: Spot high-density areas, hot spots, or outliers.\nInteractive Potential: Use tools like plotly to make maps dynamic and exploratory."
  },
  {
    "objectID": "posts/Spatial/index.html#how-to-make-them",
    "href": "posts/Spatial/index.html#how-to-make-them",
    "title": "Spatial Charts in R",
    "section": "",
    "text": "library(USAboundaries)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(plotly)\n\nwest_virginia_counties &lt;- us_counties() %&gt;%\n  select(geoid, name, state_abbr, geometry) %&gt;%  \n  filter(state_abbr == \"WV\") %&gt;%\n  mutate(population = runif(n(), 5000, 1000000)) %&gt;%  \n  mutate(pop_density = population / as.numeric(st_area(geometry))) \n\nwv_map &lt;- ggplot() +\n  geom_sf(data = virginia_counties, aes(fill = log1p(pop_density), text = name), color = \"white\") +\n  scale_fill_viridis_c(option = \"magma\", name = \"Pop Density (log)\") +\n  labs(\n    title = \"Population Density by County in West Virginia\",\n    subtitle = \"Log-transformed for better visualization\",\n    caption = \"Source: USAboundaries R package\"\n  ) +\n  theme_minimal()\n\nggplotly(wv_map, tooltip = \"text\")"
  },
  {
    "objectID": "projects/Streaming_Services/index.html#code-used-for-analysis",
    "href": "projects/Streaming_Services/index.html#code-used-for-analysis",
    "title": "Streaming Services Analysis",
    "section": "",
    "text": "import pandas as pd\nimport polars as pl\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# --- Load Data ---\ndisney = pl.read_csv(\"/Users/scotttow123/Documents/Streaming_Services/Data/disney_plus_titles.csv\")\nhulu = pl.read_csv(\"/Users/scotttow123/Documents/Streaming_Services/Data/hulu_titles.csv\")\nnetflix = pl.read_csv(\"/Users/scotttow123/Documents/Streaming_Services/Data/netflix_titles.csv\")\nprime = pd.read_csv(\"/Users/scotttow123/Documents/Streaming_Services/Data/amazon_prime_titles.csv\")\n\ndisney = disney.with_columns(pl.lit(\"Disney+\").alias(\"platform\"))\nhulu = hulu.with_columns(pl.lit(\"Hulu\").alias(\"platform\"))\nnetflix = netflix.with_columns(pl.lit(\"Netflix\").alias(\"platform\"))\n\ndisney = disney.to_pandas()\nhulu = hulu.to_pandas()\nnetflix = netflix.to_pandas()\n\nprime[\"platform\"] = \"Amazon Prime\"\n\ndata = pd.concat([disney, hulu, netflix, prime], ignore_index=True)\n\ndef convert_duration(duration):\n    if pd.isna(duration):\n        return None\n    duration = str(duration).lower()\n    if \"min\" in duration:\n        return int(duration.replace(\" min\", \"\"))\n    elif \"h\" in duration:\n        parts = duration.split(\" \")\n        hours = int(parts[0].replace(\"h\", \"\")) * 60\n        minutes = int(parts[1].replace(\"min\", \"\")) if len(parts) &gt; 1 else 0\n        return hours + minutes\n    return None\n\ndata[\"duration_minutes\"] = data[\"duration\"].apply(convert_duration)\n\ndata[\"date_added\"] = pd.to_datetime(data[\"date_added\"], errors=\"coerce\")\ndata[\"year_added\"] = data[\"date_added\"].dt.year\n\nplt.figure(figsize=(8, 6))\nsns.violinplot(data=data, x=\"platform\", y=\"duration_minutes\", palette=\"Set2\")\nplt.title(\"Distribution of Movie/Show Durations by Streaming Service\")\nplt.xlabel(\"Streaming Service\")\nplt.ylabel(\"Duration (Minutes)\")\nplt.xticks(rotation=45)\nplt.show()\n\nplt.figure(figsize=(18, 6))\ndf_yearly = data.groupby([\"year_added\", \"platform\"]).size().reset_index(name=\"count\")\nsns.lineplot(data=df_yearly, x=\"year_added\", y=\"count\", hue=\"platform\", marker=\"o\")\nplt.title(\"Number of Titles Added Over Time\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Number of Titles\")\nplt.legend(title=\"Streaming Service\")\nplt.show()\n\nplt.figure(figsize=(30, 6))\nsns.histplot(data=data, x=\"rating\", hue=\"platform\", multiple=\"stack\", palette=\"Set1\", shrink=0.8)\nplt.title(\"Distribution of Movie Ratings Across Platforms\")\nplt.xlabel(\"Rating\")\nplt.ylabel(\"Count of Titles\")\nplt.xticks(rotation=45)\nplt.legend(title=\"Streaming Service\")\nplt.show()"
  },
  {
    "objectID": "projects/Student_Performance/index.html",
    "href": "projects/Student_Performance/index.html",
    "title": "Student Performance ML",
    "section": "",
    "text": "The dataset, provided in CSV format, underwent an initial examination for missing values to ensure proper analysis. Categorical features, such as gender, race/ethnicity, and parental education level, were converted to numerical representations using one-hot encoding, improving compatibility with machine learning processing.\nNumerical features, including math, reading, and writing scores, were standardized using StandardScaler to eliminate scale biases and promote balanced model training. Additionally, a new feature, average_score, was introduced through feature engineering by calculating the mean of the three scores. This provided a target variable to enhance predictive modeling.\n\n\n\nTwo predictive models, Linear Regression and Random Forest Regressor, were used to analyze the dataset.\n\nLinear Regression serves as a baseline model, offering simplicity and interpretability.\n\nRandom Forest underwent hyperparameter tuning via GridSearchCV, optimizing parameters such as max_depth and n_estimators to enhance predictive power.\n\nThe dataset was split into training and testing subsets (80/20) to ensure an unbiased evaluation of both models. Cross-validation was applied to the Linear Regression model to improve its generalizability. Meanwhile, the Random Forest model achieved excellent performance with optimized parameters, demonstrating its reliability.\n\n\n\nModel performance was assessed using several metrics:\n\nMean Squared Error (MSE)\nR-squared\nMean Absolute Error (MAE)\nRoot Mean Squared Error (RMSE)\n\nThese evaluations confirmed the strengths of both models:\n\nLinear Regression achieved an R-squared of 0.985, indicating high reliability.\n\nRandom Forest achieved an R-squared of 0.973, validated through learning curves and cross-validation scores.\n\n\n\n\nThe results were presented using simple and clear visuals:\n\nScatter plots of actual vs.¬†predicted values showed how well each model performed.\n\nLinear Regression had points clustered more tightly than Random Forest, indicating better prediction accuracy.\n\n\nHistograms of residuals demonstrated that errors were evenly spread, with no clear patterns or biases.\n\nFeature importance analysis highlighted the most influential factors affecting performance.\n\nLearning curves for the Random Forest model showcased its performance as the dataset size increased.\n\n\n\nBelow is an example of a visualization used to assess model performance:\n\n\n\nActual vs Predicted Chart\n\n\nThe chart above compares the performance of Linear Regression and Random Forest models. Both align well with actual values, but Linear Regression shows slightly tighter clustering, indicating its predictions are closer to actual values compared to Random Forest.\n\n\n\n\n\n\nBoxplot of Distribution by Gender\n\n\nThe box plot shows a higher median math score for males, but also greater variability with more high and low outliers compared to females, who cluster closer to the average. This suggests that males are disproportionately represented at both ends of the math achievement spectrum, raising questions about contributing factors and the need for equitable educational practices.\n\n\n\n\n\n\nHistogram\n\n\nThe histograms reveal that math scores exhibit a more left-skewed distribution, indicating a tendency towards lower scores, while reading and writing scores are more normally distributed, suggesting a wider range of performance centered around the average. Notably, writing scores show a slight right skew, implying a potential ceiling effect or a higher proportion of students achieving higher scores in writing.\n\n\n\n\nBias analysis examined potential differences in math scores between genders to ensure fairness in the model‚Äôs predictions.\n\nThe model produced an average score of 63.63 for females and 68.72 for males.\n\nThese findings highlight the importance of addressing potential biases in machine learning models to prevent reinforcement of societal inequalities.\n\nThe analysis confirmed that the model does not introduce biases, as the differences in scores align with the dataset‚Äôs inherent distribution.\n\n\n\n\nColab Notebook:\nPersonal Project Link"
  },
  {
    "objectID": "projects/Student_Performance/index.html#i.-data-preprocessing",
    "href": "projects/Student_Performance/index.html#i.-data-preprocessing",
    "title": "Student Performance ML",
    "section": "",
    "text": "The dataset, provided in CSV format, underwent an initial examination for missing values to ensure proper analysis. Categorical features, such as gender, race/ethnicity, and parental education level, were converted to numerical representations using one-hot encoding, improving compatibility with machine learning processing.\nNumerical features, including math, reading, and writing scores, were standardized using StandardScaler to eliminate scale biases and promote balanced model training. Additionally, a new feature, average_score, was introduced through feature engineering by calculating the mean of the three scores. This provided a target variable to enhance predictive modeling."
  },
  {
    "objectID": "projects/Student_Performance/index.html#ii.-selection-training-and-ml-model",
    "href": "projects/Student_Performance/index.html#ii.-selection-training-and-ml-model",
    "title": "Student Performance ML",
    "section": "",
    "text": "Two predictive models, Linear Regression and Random Forest Regressor, were used to analyze the dataset.\n\nLinear Regression serves as a baseline model, offering simplicity and interpretability.\n\nRandom Forest underwent hyperparameter tuning via GridSearchCV, optimizing parameters such as max_depth and n_estimators to enhance predictive power.\n\nThe dataset was split into training and testing subsets (80/20) to ensure an unbiased evaluation of both models. Cross-validation was applied to the Linear Regression model to improve its generalizability. Meanwhile, the Random Forest model achieved excellent performance with optimized parameters, demonstrating its reliability."
  },
  {
    "objectID": "projects/Student_Performance/index.html#iii.-interpretation-of-results",
    "href": "projects/Student_Performance/index.html#iii.-interpretation-of-results",
    "title": "Student Performance ML",
    "section": "",
    "text": "Model performance was assessed using several metrics:\n\nMean Squared Error (MSE)\nR-squared\nMean Absolute Error (MAE)\nRoot Mean Squared Error (RMSE)\n\nThese evaluations confirmed the strengths of both models:\n\nLinear Regression achieved an R-squared of 0.985, indicating high reliability.\n\nRandom Forest achieved an R-squared of 0.973, validated through learning curves and cross-validation scores."
  },
  {
    "objectID": "projects/Student_Performance/index.html#iv.-communication-of-results",
    "href": "projects/Student_Performance/index.html#iv.-communication-of-results",
    "title": "Student Performance ML",
    "section": "",
    "text": "The results were presented using simple and clear visuals:\n\nScatter plots of actual vs.¬†predicted values showed how well each model performed.\n\nLinear Regression had points clustered more tightly than Random Forest, indicating better prediction accuracy.\n\n\nHistograms of residuals demonstrated that errors were evenly spread, with no clear patterns or biases.\n\nFeature importance analysis highlighted the most influential factors affecting performance.\n\nLearning curves for the Random Forest model showcased its performance as the dataset size increased.\n\n\n\nBelow is an example of a visualization used to assess model performance:\n\n\n\nActual vs Predicted Chart\n\n\nThe chart above compares the performance of Linear Regression and Random Forest models. Both align well with actual values, but Linear Regression shows slightly tighter clustering, indicating its predictions are closer to actual values compared to Random Forest.\n\n\n\n\n\n\nBoxplot of Distribution by Gender\n\n\nThe box plot shows a higher median math score for males, but also greater variability with more high and low outliers compared to females, who cluster closer to the average. This suggests that males are disproportionately represented at both ends of the math achievement spectrum, raising questions about contributing factors and the need for equitable educational practices.\n\n\n\n\n\n\nHistogram\n\n\nThe histograms reveal that math scores exhibit a more left-skewed distribution, indicating a tendency towards lower scores, while reading and writing scores are more normally distributed, suggesting a wider range of performance centered around the average. Notably, writing scores show a slight right skew, implying a potential ceiling effect or a higher proportion of students achieving higher scores in writing."
  },
  {
    "objectID": "projects/Student_Performance/index.html#v.-ethical-implications",
    "href": "projects/Student_Performance/index.html#v.-ethical-implications",
    "title": "Student Performance ML",
    "section": "",
    "text": "Bias analysis examined potential differences in math scores between genders to ensure fairness in the model‚Äôs predictions.\n\nThe model produced an average score of 63.63 for females and 68.72 for males.\n\nThese findings highlight the importance of addressing potential biases in machine learning models to prevent reinforcement of societal inequalities.\n\nThe analysis confirmed that the model does not introduce biases, as the differences in scores align with the dataset‚Äôs inherent distribution."
  },
  {
    "objectID": "projects/Student_Performance/index.html#vi.-python-notebooks",
    "href": "projects/Student_Performance/index.html#vi.-python-notebooks",
    "title": "Student Performance ML",
    "section": "",
    "text": "Colab Notebook:\nPersonal Project Link"
  }
]