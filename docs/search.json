[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Scott Townsend",
    "section": "",
    "text": "About me:\nHi there! My name is Scott Townsend, and Iâ€™m a senior majoring in Data Science at Brigham Young University - Idaho. Currently, Iâ€™m working on my Senior Data Science Project, which has been an exciting and challenging journey.\nThrough this blog, Iâ€™ll be sharing updates on my project as I progress, along with any insights and challenges I face along the way. Iâ€™ll also showcase other data science projects Iâ€™ve completed, as well as those Iâ€™m actively working on, ranging from statistical analyses to machine learning applications."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/Week_01/index.html",
    "href": "posts/Week_01/index.html",
    "title": "Week 1",
    "section": "",
    "text": "As part of my Senior Data Science Project, Iâ€™m creating a tool that automatically generates captions for images. This tool will help visually impaired individuals understand pictures and streamline tasks like photo tagging and social media captioning.\nThe model will be trained on the Flickr8k dataset, which contains thousands of images paired with captions. The goal is to build a model that can look at an image and generate a relevant description using machine learning techniques.\n\n\n\nThe following deliverables are part of this project:\n\nCleaned Data: Organize the dataset by ensuring that images and captions are properly matched.\nWorking Model: A model that extracts features from images (CNN) and generates captions (LSTM/Transformer).\nPerformance Check: Evaluate how well the captions align with the images.\nInteractive Demo: A tool to upload an image and receive a generated caption.\nVisuals: Graphs and diagrams showing the modelâ€™s performance and where it focuses in the image.\nWrite-Up: A brief explanation of the project and its potential applications.\n\n\n\n\nFirst, I load the dataset and ensure that the images and captions are properly matched:\nimport pandas as pd\nimport os\n\n# Load dataset\ncaption_file = \"/path/to/captions.txt\"\ntry:\n    data = pd.read_csv(caption_file)\n    print(\"Data loaded successfully\")\nexcept FileNotFoundError:\n    print(f\"Error: The file {caption_file} was not found.\")\n    data = pd.DataFrame()\n\ndata.head()\n\n# Remove duplicates\nif data.duplicated().any():\n    print(f\"Found {data.duplicated().sum()} duplicate rows. Removing duplicates...\")\n    data = data.drop_duplicates()\n\n# Handle missing captions\nif data['caption'].isnull().any():\n    print(f\"Found {data['caption'].isnull().sum()} missing captions. Replacing with 'No caption'.\")\n    data['caption'] = data['caption'].fillna(\"No caption\")\n\n# Check for invalid image paths\nvalid_image_paths = data['image'].apply(lambda x: os.path.exists(os.path.join(image_path, x)))\nif not valid_image_paths.all():\n    print(f\"Found {(~valid_image_paths).sum()} invalid image paths. Removing these rows...\")\n    data = data[valid_image_paths]\nStay tuned for the next post where Iâ€™ll dive into the data preprocessing for text and image features!"
  },
  {
    "objectID": "posts/Week_01/index.html#introduction",
    "href": "posts/Week_01/index.html#introduction",
    "title": "Week 1",
    "section": "",
    "text": "As part of my Senior Data Science Project, Iâ€™m creating a tool that automatically generates captions for images. This tool will help visually impaired individuals understand pictures and streamline tasks like photo tagging and social media captioning.\nThe model will be trained on the Flickr8k dataset, which contains thousands of images paired with captions. The goal is to build a model that can look at an image and generate a relevant description using machine learning techniques."
  },
  {
    "objectID": "posts/Week_01/index.html#project-deliverables",
    "href": "posts/Week_01/index.html#project-deliverables",
    "title": "Week 1",
    "section": "",
    "text": "The following deliverables are part of this project:\n\nCleaned Data: Organize the dataset by ensuring that images and captions are properly matched.\nWorking Model: A model that extracts features from images (CNN) and generates captions (LSTM/Transformer).\nPerformance Check: Evaluate how well the captions align with the images.\nInteractive Demo: A tool to upload an image and receive a generated caption.\nVisuals: Graphs and diagrams showing the modelâ€™s performance and where it focuses in the image.\nWrite-Up: A brief explanation of the project and its potential applications."
  },
  {
    "objectID": "posts/Week_01/index.html#code-example-data-loading-and-preprocessing",
    "href": "posts/Week_01/index.html#code-example-data-loading-and-preprocessing",
    "title": "Week 1",
    "section": "",
    "text": "First, I load the dataset and ensure that the images and captions are properly matched:\nimport pandas as pd\nimport os\n\n# Load dataset\ncaption_file = \"/path/to/captions.txt\"\ntry:\n    data = pd.read_csv(caption_file)\n    print(\"Data loaded successfully\")\nexcept FileNotFoundError:\n    print(f\"Error: The file {caption_file} was not found.\")\n    data = pd.DataFrame()\n\ndata.head()\n\n# Remove duplicates\nif data.duplicated().any():\n    print(f\"Found {data.duplicated().sum()} duplicate rows. Removing duplicates...\")\n    data = data.drop_duplicates()\n\n# Handle missing captions\nif data['caption'].isnull().any():\n    print(f\"Found {data['caption'].isnull().sum()} missing captions. Replacing with 'No caption'.\")\n    data['caption'] = data['caption'].fillna(\"No caption\")\n\n# Check for invalid image paths\nvalid_image_paths = data['image'].apply(lambda x: os.path.exists(os.path.join(image_path, x)))\nif not valid_image_paths.all():\n    print(f\"Found {(~valid_image_paths).sum()} invalid image paths. Removing these rows...\")\n    data = data[valid_image_paths]\nStay tuned for the next post where Iâ€™ll dive into the data preprocessing for text and image features!"
  },
  {
    "objectID": "posts/Week_03/index.html",
    "href": "posts/Week_03/index.html",
    "title": "Week 3",
    "section": "",
    "text": "In this post, Iâ€™ll walk you through the architecture of the model that generates captions for images.\n\n\nI start by using a pre-trained DenseNet201 model for feature extraction from the images. This CNN extracts useful features from the images, which will then be used by the LSTM model to generate captions.\nfrom tensorflow.keras.applications import DenseNet201\nfrom tensorflow.keras.models import Model\nfrom tqdm import tqdm\n\nmodel = DenseNet201()\nfe = Model(inputs=model.input, outputs=model.layers[-2].output)\n\nimg_size = 224\nfeatures = {}\nfor image in tqdm(data['image'].unique().tolist()):\n    img = load_img(os.path.join(image_path, image), target_size=(img_size, img_size))\n    img = img_to_array(img)\n    img = img / 255.\n    img = np.expand_dims(img, axis=0)\n    feature = fe.predict(img, verbose=0)\n    features[image] = feature\n\n\n\nThe model consists of two inputs: one for the image features and another for the tokenized caption. The image features go through a dense layer, and the caption goes through an embedding layer and an LSTM. The outputs are merged, and the model learns to predict the next word in the caption.\nfrom tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add, concatenate, Reshape\n\ninput1 = Input(shape=(1920,))  # Image features from CNN\ninput2 = Input(shape=(max_length,))  # Tokenized caption\n\nimg_features = Dense(256, activation='relu')(input1)\nimg_features_reshaped = Reshape((1, 256))(img_features)\n\nsentence_features = Embedding(vocab_size, 256)(input2)\nmerged = concatenate([img_features_reshaped, sentence_features], axis=1)\nsentence_features = LSTM(256)(merged)\nx = Dropout(0.5)(sentence_features)\nx = add([x, img_features])\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.5)(x)\noutput = Dense(vocab_size, activation='softmax')(x)\n\ncaption_model = Model(inputs=[input1, input2], outputs=output)\ncaption_model.compile(loss='categorical_crossentropy', optimizer='adam')\n\n\n\nI use the CustomDataGenerator to feed the data into the model, and the model is trained with checkpoints, early stopping, and learning rate reduction to avoid overfitting.\nhistory = caption_model.fit(\n        train_generator,\n        epochs=50,\n        validation_data=validation_generator,\n        callbacks=[checkpoint, earlystopping, learning_rate_reduction]\n)\n\n\n\nOnce the model is trained, I can use it to generate captions for unseen images. In the next post, Iâ€™ll show how the model performs and how it generates captions for test images.\nStay tuned for more insights into how the model works and its real-world applications!"
  },
  {
    "objectID": "posts/Week_03/index.html#model-architecture",
    "href": "posts/Week_03/index.html#model-architecture",
    "title": "Week 3",
    "section": "",
    "text": "The model consists of two inputs: one for the image features and another for the tokenized caption. The image features go through a dense layer, and the caption goes through an embedding layer and an LSTM. The outputs are merged, and the model learns to predict the next word in the caption.\nfrom tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add, concatenate, Reshape\n\ninput1 = Input(shape=(1920,))  # Image features from CNN\ninput2 = Input(shape=(max_length,))  # Tokenized caption\n\nimg_features = Dense(256, activation='relu')(input1)\nimg_features_reshaped = Reshape((1, 256))(img_features)\n\nsentence_features = Embedding(vocab_size, 256)(input2)\nmerged = concatenate([img_features_reshaped, sentence_features], axis=1)\nsentence_features = LSTM(256)(merged)\nx = Dropout(0.5)(sentence_features)\nx = add([x, img_features])\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.5)(x)\noutput = Dense(vocab_size, activation='softmax')(x)\n\ncaption_model = Model(inputs=[input1, input2], outputs=output)\ncaption_model.compile(loss='categorical_crossentropy', optimizer='adam')"
  },
  {
    "objectID": "posts/Week_03/index.html#training-the-model",
    "href": "posts/Week_03/index.html#training-the-model",
    "title": "Week 3",
    "section": "",
    "text": "I use the CustomDataGenerator to feed the data into the model, and the model is trained with checkpoints, early stopping, and learning rate reduction to avoid overfitting.\nhistory = caption_model.fit(\n        train_generator,\n        epochs=50,\n        validation_data=validation_generator,\n        callbacks=[checkpoint, earlystopping, learning_rate_reduction]\n)"
  },
  {
    "objectID": "posts/Week_03/index.html#final-thoughts",
    "href": "posts/Week_03/index.html#final-thoughts",
    "title": "Week 3",
    "section": "",
    "text": "Once the model is trained, I can use it to generate captions for unseen images. In the next post, Iâ€™ll show how the model performs and how it generates captions for test images.\nStay tuned for more insights into how the model works and its real-world applications!"
  },
  {
    "objectID": "posts/Week_02/index.html",
    "href": "posts/Week_02/index.html",
    "title": "Week 2",
    "section": "",
    "text": "In this post, Iâ€™ll go through the essential preprocessing steps I took to prepare the image and caption data for the model.\n\n\nFor each image, I resize it and normalize the pixel values to a range between 0 and 1:\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n\ndef readImage(path, img_size=224):\n    img = load_img(path, color_mode='rgb', target_size=(img_size, img_size))\n    img = img_to_array(img)\n    img = img / 255.  # Normalize\n    return img\n\ndef text_preprocessing(data):\n    data['caption'] = data['caption'].apply(lambda x: x.lower())\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\", \"\"))\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\", \" \"))\n    data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word) &gt; 1]))\n    data['caption'] = \"startseq \" + data['caption'] + \" endseq\"\n    return data\n\ndata = text_preprocessing(data)\ncaptions = data['caption'].tolist()\ncaptions[:10]\nNext, I will tokenize the captions, splitting them into words and assigning each word a unique integer.\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(captions)\nvocab_size = len(tokenizer.word_index) + 1  # Include padding token\nmax_length = max(len(caption.split()) for caption in captions)\n\n# Split into train and test\nimages = data['image'].unique().tolist()\nsplit_index = round(0.85 * len(images))\ntrain_images = images[:split_index]\nval_images = images[split_index:]\n\ntrain = data[data['image'].isin(train_images)]\ntest = data[data['image'].isin(val_images)]"
  },
  {
    "objectID": "posts/Week_02/index.html#image-preprocessing",
    "href": "posts/Week_02/index.html#image-preprocessing",
    "title": "Week 2",
    "section": "",
    "text": "For each image, I resize it and normalize the pixel values to a range between 0 and 1:\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n\ndef readImage(path, img_size=224):\n    img = load_img(path, color_mode='rgb', target_size=(img_size, img_size))\n    img = img_to_array(img)\n    img = img / 255.  # Normalize\n    return img\n\ndef text_preprocessing(data):\n    data['caption'] = data['caption'].apply(lambda x: x.lower())\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\", \"\"))\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\", \" \"))\n    data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word) &gt; 1]))\n    data['caption'] = \"startseq \" + data['caption'] + \" endseq\"\n    return data\n\ndata = text_preprocessing(data)\ncaptions = data['caption'].tolist()\ncaptions[:10]\nNext, I will tokenize the captions, splitting them into words and assigning each word a unique integer.\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(captions)\nvocab_size = len(tokenizer.word_index) + 1  # Include padding token\nmax_length = max(len(caption.split()) for caption in captions)\n\n# Split into train and test\nimages = data['image'].unique().tolist()\nsplit_index = round(0.85 * len(images))\ntrain_images = images[:split_index]\nval_images = images[split_index:]\n\ntrain = data[data['image'].isin(train_images)]\ntest = data[data['image'].isin(val_images)]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Senior Project Blog",
    "section": "",
    "text": "Week 3\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 11, 2025\n\n\nScott Townsend\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 4, 2025\n\n\nScott Townsend\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 1\n\n\n\n\n\n\nnews\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 27, 2025\n\n\nScott Townsend\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Criminal Incident Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html#projects",
    "href": "projects.html#projects",
    "title": "ðŸ“š Projects",
    "section": "",
    "text": "A collection of my personal projects showcasing my work in data analysis, statistical modeling, and machine learning.\n\n\n\nStreaming Services Data Analysis (January 2025 - Present)\nAnalyzed Netflix, Hulu, Disney+, and Amazon content libraries to uncover trends in movie durations, genres, and ratings.\n\nUtilized Polars and Pandas for efficient data processing.\n\nVisualized insights with Seaborn, Matplotlib, and Plotly, exploring genre popularity, rating distributions, and country-wise content production.\n\nBuilt interactive dashboards and statistical summaries to inform data-driven decisions.\n\nImage Captioning Tool for Visually Impaired Users (December 2024 - Present)\nDeveloped a deep learning model to generate descriptive captions for images, aiding visually impaired individuals and automating social media captioning.\n\nImplemented a CNN (VGG16) for image feature extraction and an LSTM/Transformer for caption generation.\n\nVisualized model attention areas using heatmaps and presented insights through data visualizations.\n\nCrime Analysis\nExploring crime trends through data analysis and visualization.\nStudent Performance Analysis\nAnalyzing factors affecting student academic performance.\nStudent Performance Predictive ML Model\nBuilding a predictive model to forecast student success based on historical data.\nPopulation Growth Analysis\nInvestigating population growth trends using statistical methods.\n\nðŸ’¡ Feel free to explore my projects for detailed analysis and visualization."
  },
  {
    "objectID": "projects.html#my-current-projects",
    "href": "projects.html#my-current-projects",
    "title": "ðŸ“š Projects",
    "section": "",
    "text": "A collection of my personal projects showcasing my work in data analysis, statistical modeling, and machine learning.\n\n\n\nStreaming Services Data Analysis (January 2025 - Present)\nAnalyzed Netflix, Hulu, Disney+, and Amazon content libraries to uncover trends in movie durations, genres, and ratings.\n\nUtilized Polars and Pandas for efficient data processing.\n\nVisualized insights with Seaborn, Matplotlib, and Plotly, exploring genre popularity, rating distributions, and country-wise content production.\n\nBuilt interactive dashboards and statistical summaries to inform data-driven decisions.\n\nImage Captioning Tool for Visually Impaired Users (December 2024 - Present)\nDeveloped a deep learning model to generate descriptive captions for images, aiding visually impaired individuals and automating social media captioning.\n\nImplemented a CNN (VGG16) for image feature extraction and an LSTM/Transformer for caption generation.\n\nVisualized model attention areas using heatmaps and presented insights through data visualizations.\n\nCrime Analysis\nExploring crime trends through data analysis and visualization. Developed interactive visualizations in Python (Matplotlib, Plotly) to highlight trends in the demographics of offenders and victims.\n\nApplied data wrangling techniques to identify patterns and trends, offering actionable insights into public safety strategies.\nAnalyzed offender-victim relationships to uncover societal trends influencing crime rates.\n\nStudent Performance Analysis\nAnalyzing factors affecting student academic performance.\nStudent Performance Predictive ML Model\nBuilding a predictive model to forecast student success based on historical data.\nPopulation Growth Analysis\nInvestigating population growth trends using statistical methods.\n\nðŸ’¡ Feel free to explore my projects for detailed analysis and visualization."
  },
  {
    "objectID": "projects/Crime_Analysis/index.html",
    "href": "projects/Crime_Analysis/index.html",
    "title": "Criminal Incident Analysis",
    "section": "",
    "text": "The analysis is structured into several key sections, each exploring a different aspect of criminal incidents. Below are the main insights drawn from the data:\n\n\nThis visualization provides an overview of the frequency of criminal incidents based on their location. Understanding where crimes occur helps in targeting both public and private safety initiatives.\n\n\n\n\nResidential areas are the most common settings for criminal incidents, followed closely by public locations such as streets and sidewalks. These findings underscore the need for improved safety measures in both private homes and public spaces.\n\n\n\n\n\nThe following charts show the age distributions of both offenders and victims, offering insights into the typical age groups involved in these incidents.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOffenders: The most common age group among offenders is 20â€“29, followed by 30â€“39. This indicates that younger adults are more likely to be involved in criminal activities.\nVictims: Victimization also peaks in the 20â€“29 age group, suggesting a strong overlap between the ages of offenders and victims. This may reflect social or environmental factors influencing both offending and victimization patterns.\n\n\n\n\n\nThe charts below compare the ethnicities of offenders and victims to reveal potential patterns of racial disparities in criminal incidents.\n\n\n\nEthnicties of the Offenders\n\n\n\n\n\n\n\n\n\n\nThe majority of both offenders and victims are categorized as non-Hispanic and non-Latino. This suggests that ethnic groups outside of the Hispanic/Latino community are more frequently involved in criminal incidents. Further analysis is needed to explore the sociocultural and systemic factors behind these patterns.\n\n\n\n\n\nThis analysis explores the racial distribution of offenders and victims, investigating whether race-related disparities are present.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOffenders: Both White and African American individuals are most commonly represented among offenders, highlighting possible broader societal and economic influences.\nVictims: White individuals appear to be more frequently victimized, suggesting potential interactions between race, geographic location, and social dynamics. This warrants further investigation into how systemic factors contribute to these disparities.\n\n\n\n\n\nThis chart categorizes the types of offenses involved in the reported incidents, giving us a detailed breakdown of criminal activity.\n\n\n\n\n\n\n\n\nProperty destruction is the most prevalent offense, followed by simple assaults, breaking and entering, and drug-related crimes. This breakdown provides a clearer picture of the types of criminal activities most commonly occurring in these incidents.\n\n\n\n\n\nThe following chart provides an overview of the types of weapons used during criminal incidents, offering valuable insights into the role of weaponry in violent crime.\n\n\n\n\n\n\n\n\nPersonal weapons (hands, fists, etc.) are the most frequently used in incidents, followed by firearms and knives. The prominence of physical altercations emphasizes the need for preventive measures around conflict de-escalation, while the prevalence of firearms and knives highlights the importance of effective weapon control.\n\n\n\n\n\nThis chart examines the relationship dynamics between offenders and victims, uncovering patterns in how these relationships might influence criminal behavior.\n\n\n\n\n\n\n\n\nThe most common relationship between offenders and victims is that of strangers, where no prior connection exists between the parties. Other significant relationships include boyfriends/girlfriends, friends, spouses, and acquaintances. This indicates a range of scenarios, from random violence to incidents stemming from interpersonal conflicts.\n\n\n\n\n\n\nThis analysis uncovers several critical patterns in criminal activity related to gun violence. Key findings include:\n\nYounger individuals, particularly in their 20s and 30s, are more likely to be involved both as offenders and victims.\nThe most common offenses involve property destruction, assaults, and drug-related crimes.\nBoth White and African American individuals are most commonly represented among offenders, while White individuals are more frequently victimized.\nPersonal weapons and firearms are the most common weapons used in violent incidents.\n\nThese insights are crucial for understanding the social dynamics behind gun violence. By further exploring these trends and integrating additional datasets, law enforcement agencies and policymakers can make more informed decisions about crime prevention, resource allocation, and public safety initiatives."
  },
  {
    "objectID": "projects/Crime_Analysis/index.html#analysis-overview",
    "href": "projects/Crime_Analysis/index.html#analysis-overview",
    "title": "Criminal Incident Analysis",
    "section": "",
    "text": "The analysis is structured into several key sections, each exploring a different aspect of criminal incidents. Below are the main insights drawn from the data:\n\n\nThis visualization provides an overview of the frequency of criminal incidents based on their location. Understanding where crimes occur helps in targeting both public and private safety initiatives.\n\n\n\n\nResidential areas are the most common settings for criminal incidents, followed closely by public locations such as streets and sidewalks. These findings underscore the need for improved safety measures in both private homes and public spaces.\n\n\n\n\n\nThe following charts show the age distributions of both offenders and victims, offering insights into the typical age groups involved in these incidents.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOffenders: The most common age group among offenders is 20â€“29, followed by 30â€“39. This indicates that younger adults are more likely to be involved in criminal activities.\nVictims: Victimization also peaks in the 20â€“29 age group, suggesting a strong overlap between the ages of offenders and victims. This may reflect social or environmental factors influencing both offending and victimization patterns.\n\n\n\n\n\nThe charts below compare the ethnicities of offenders and victims to reveal potential patterns of racial disparities in criminal incidents.\n\n\n\nEthnicties of the Offenders\n\n\n\n\n\n\n\n\n\n\nThe majority of both offenders and victims are categorized as non-Hispanic and non-Latino. This suggests that ethnic groups outside of the Hispanic/Latino community are more frequently involved in criminal incidents. Further analysis is needed to explore the sociocultural and systemic factors behind these patterns.\n\n\n\n\n\nThis analysis explores the racial distribution of offenders and victims, investigating whether race-related disparities are present.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOffenders: Both White and African American individuals are most commonly represented among offenders, highlighting possible broader societal and economic influences.\nVictims: White individuals appear to be more frequently victimized, suggesting potential interactions between race, geographic location, and social dynamics. This warrants further investigation into how systemic factors contribute to these disparities.\n\n\n\n\n\nThis chart categorizes the types of offenses involved in the reported incidents, giving us a detailed breakdown of criminal activity.\n\n\n\n\n\n\n\n\nProperty destruction is the most prevalent offense, followed by simple assaults, breaking and entering, and drug-related crimes. This breakdown provides a clearer picture of the types of criminal activities most commonly occurring in these incidents.\n\n\n\n\n\nThe following chart provides an overview of the types of weapons used during criminal incidents, offering valuable insights into the role of weaponry in violent crime.\n\n\n\n\n\n\n\n\nPersonal weapons (hands, fists, etc.) are the most frequently used in incidents, followed by firearms and knives. The prominence of physical altercations emphasizes the need for preventive measures around conflict de-escalation, while the prevalence of firearms and knives highlights the importance of effective weapon control.\n\n\n\n\n\nThis chart examines the relationship dynamics between offenders and victims, uncovering patterns in how these relationships might influence criminal behavior.\n\n\n\n\n\n\n\n\nThe most common relationship between offenders and victims is that of strangers, where no prior connection exists between the parties. Other significant relationships include boyfriends/girlfriends, friends, spouses, and acquaintances. This indicates a range of scenarios, from random violence to incidents stemming from interpersonal conflicts."
  },
  {
    "objectID": "projects/Crime_Analysis/index.html#conclusion",
    "href": "projects/Crime_Analysis/index.html#conclusion",
    "title": "Criminal Incident Analysis",
    "section": "",
    "text": "This analysis uncovers several critical patterns in criminal activity related to gun violence. Key findings include:\n\nYounger individuals, particularly in their 20s and 30s, are more likely to be involved both as offenders and victims.\nThe most common offenses involve property destruction, assaults, and drug-related crimes.\nBoth White and African American individuals are most commonly represented among offenders, while White individuals are more frequently victimized.\nPersonal weapons and firearms are the most common weapons used in violent incidents.\n\nThese insights are crucial for understanding the social dynamics behind gun violence. By further exploring these trends and integrating additional datasets, law enforcement agencies and policymakers can make more informed decisions about crime prevention, resource allocation, and public safety initiatives."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Senior Project Blog",
    "section": "",
    "text": "Week 3\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 11, 2025\n\n\nScott Townsend\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 4, 2025\n\n\nScott Townsend\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 1\n\n\n\n\n\n\nnews\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 27, 2025\n\n\nScott Townsend\n\n\n\n\n\n\nNo matching items"
  }
]