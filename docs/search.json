[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Scott Townsend",
    "section": "",
    "text": "Hi there! My name is Scott Townsend, and I’m a Data Scientist with a strong background in data science and machine learning. I’ve worked on various projects ranging from statistical analyses to building machine learning models, and I’m passionate about using data to drive actionable insights.\nThrough this blog, I’ll be sharing updates on my ongoing projects, insights, and challenges I encounter along the way. I’ll also showcase data science projects I’ve completed, highlighting the tools, techniques, and results from each."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/Week_01/index.html",
    "href": "posts/Week_01/index.html",
    "title": "Week 1",
    "section": "",
    "text": "As part of my Senior Data Science Project, I’m creating a tool that automatically generates captions for images. This tool aims to help visually impaired individuals understand pictures and streamline tasks like photo tagging and social media captioning.\nThe model will be trained on the Flickr8k dataset, which contains thousands of images paired with captions. The goal is to build a model that can analyze an image and generate a relevant description using machine learning techniques.\n\n\n\nThe following deliverables are part of this project:\n\nCleaned Data: Organize the dataset by ensuring that images and captions are properly matched.\nWorking Model: A model that extracts features from images (CNN) and generates captions (LSTM/Transformer).\nPerformance Check: Evaluate how well the captions align with the images.\nVisuals: Graphs and diagrams showing the model’s performance and where it focuses in the image.\nWrite-Up: A brief explanation of the project and its potential applications.\n\n\n\n\nFirst, I load the dataset and ensure that the images and captions are properly matched:\nimport pandas as pd\nimport os\n\nimage_path = '/Users/scotttow123/Documents/BYUI/Senior_Project/Images'\ncaption_file = '/Users/scotttow123/Documents/BYUI/Senior_Project/captions.txt'\n\ntry:\n    data = pd.read_csv(caption_file)\n    print(\"Data loaded successfully\")\nexcept FileNotFoundError:\n    print(f\"Error: The file {caption_file} was not found.\")\n    data = pd.DataFrame()\n\ndata.head()\nNext, I remove any duplicates and handle missing captions:\n# Remove duplicates\nif data.duplicated().any():\n    print(f\"Found {data.duplicated().sum()} duplicate rows. Removing duplicates...\")\n    data = data.drop_duplicates()\n\n# Handle missing captions\nif data['caption'].isnull().any():\n    print(f\"Found {data['caption'].isnull().sum()} missing captions. Replacing with 'No caption'.\")\n    data['caption'] = data['caption'].fillna(\"No caption\")\n\n# Validate image paths\nvalid_image_paths = data['image'].apply(lambda x: os.path.exists(os.path.join(image_path, x)))\nif not valid_image_paths.all():\n    print(f\"Found {(~valid_image_paths).sum()} invalid image paths. Removing these rows...\")\n    data = data[valid_image_paths]\nThe code above ran successfully, indicating:\nFound 10 duplicate rows. Removing duplicates...\nThis confirmed that there were duplicates within the Flickr dataset.\nTo visualize a sample of images with their captions, I used:\ndisplay_images(data.sample(15))\nThis line of code shows a random sample of 15 images with their respective captions.\n\n\n\nRandom Sample of Images\n\n\n\n\n\nFinally, I implemented text preprocessing to clean and format the captions:\ndef text_preprocessing(data):\n    data['caption'] = data['caption'].apply(lambda x: x.lower())\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\", \"\"))\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\", \" \"))\n    data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word) &gt; 1]))\n    data['caption'] = \"startseq \" + data['caption'] + \" endseq\"\n    return data\n\ndata = text_preprocessing(data)\ncaptions = data['caption'].tolist()\ncaptions[:10]\n['startseq child in pink dress is climbing up set of stairs in an entry way endseq',\n 'startseq girl going into wooden building endseq',\n 'startseq little girl climbing into wooden playhouse endseq',\n 'startseq little girl climbing the stairs to her playhouse endseq',\n 'startseq little girl in pink dress going into wooden cabin endseq',\n 'startseq black dog and spotted dog are fighting endseq',\n 'startseq black dog and tri-colored dog playing with each other on the road endseq',\n 'startseq black dog and white dog with brown spots are staring at each other in the street endseq',\n 'startseq two dogs of different breeds looking at each other on the road endseq',\n 'startseq two dogs on pavement moving toward each other endseq']\nThis code ensures that captions are converted to lowercase, unwanted characters are removed, and sequences are formatted with start and end tokens. Above we can see a snippet of what this looks like.\n\nWith the data cleaned and preprocessed, I am now ready to move on to building and training the image captioning model. More updates to come!"
  },
  {
    "objectID": "posts/Week_01/index.html#introduction",
    "href": "posts/Week_01/index.html#introduction",
    "title": "Week 1",
    "section": "",
    "text": "As part of my Senior Data Science Project, I’m creating a tool that automatically generates captions for images. This tool aims to help visually impaired individuals understand pictures and streamline tasks like photo tagging and social media captioning.\nThe model will be trained on the Flickr8k dataset, which contains thousands of images paired with captions. The goal is to build a model that can analyze an image and generate a relevant description using machine learning techniques."
  },
  {
    "objectID": "posts/Week_01/index.html#project-deliverables",
    "href": "posts/Week_01/index.html#project-deliverables",
    "title": "Week 1",
    "section": "",
    "text": "The following deliverables are part of this project:\n\nCleaned Data: Organize the dataset by ensuring that images and captions are properly matched.\nWorking Model: A model that extracts features from images (CNN) and generates captions (LSTM/Transformer).\nPerformance Check: Evaluate how well the captions align with the images.\nVisuals: Graphs and diagrams showing the model’s performance and where it focuses in the image.\nWrite-Up: A brief explanation of the project and its potential applications."
  },
  {
    "objectID": "posts/Week_01/index.html#code-example-data-loading-and-preprocessing",
    "href": "posts/Week_01/index.html#code-example-data-loading-and-preprocessing",
    "title": "Week 1",
    "section": "",
    "text": "First, I load the dataset and ensure that the images and captions are properly matched:\nimport pandas as pd\nimport os\n\nimage_path = '/Users/scotttow123/Documents/BYUI/Senior_Project/Images'\ncaption_file = '/Users/scotttow123/Documents/BYUI/Senior_Project/captions.txt'\n\ntry:\n    data = pd.read_csv(caption_file)\n    print(\"Data loaded successfully\")\nexcept FileNotFoundError:\n    print(f\"Error: The file {caption_file} was not found.\")\n    data = pd.DataFrame()\n\ndata.head()\nNext, I remove any duplicates and handle missing captions:\n# Remove duplicates\nif data.duplicated().any():\n    print(f\"Found {data.duplicated().sum()} duplicate rows. Removing duplicates...\")\n    data = data.drop_duplicates()\n\n# Handle missing captions\nif data['caption'].isnull().any():\n    print(f\"Found {data['caption'].isnull().sum()} missing captions. Replacing with 'No caption'.\")\n    data['caption'] = data['caption'].fillna(\"No caption\")\n\n# Validate image paths\nvalid_image_paths = data['image'].apply(lambda x: os.path.exists(os.path.join(image_path, x)))\nif not valid_image_paths.all():\n    print(f\"Found {(~valid_image_paths).sum()} invalid image paths. Removing these rows...\")\n    data = data[valid_image_paths]\nThe code above ran successfully, indicating:\nFound 10 duplicate rows. Removing duplicates...\nThis confirmed that there were duplicates within the Flickr dataset.\nTo visualize a sample of images with their captions, I used:\ndisplay_images(data.sample(15))\nThis line of code shows a random sample of 15 images with their respective captions.\n\n\n\nRandom Sample of Images"
  },
  {
    "objectID": "posts/Week_03/index.html",
    "href": "posts/Week_03/index.html",
    "title": "Week 3",
    "section": "",
    "text": "Word Cloud\nBelow is a visualization of the most popular words from the caption dataset. Notably, “Man” and “Woman” emerge as the most frequently mentioned words within the captions.\n\n\n\nWord Cloud\n\n\n\n\nCaption Model Architecture\nBelow is the code for defining and compiling a neural network model designed for image captioning. The model combines image and text features, processes them through LSTM layers, and generates captions.\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Reshape, Embedding, LSTM, Dropout, add, concatenate\nfrom keras.utils import plot_model\n\n# Define input layers\ninput1 = Input(shape=(1920,))\ninput2 = Input(shape=(max_length,))\n\n# Image feature layers\nimg_features = Dense(256, activation='relu')(input1)\nimg_features_reshaped = Reshape((1, 256), input_shape=(256,))(img_features)\n\n# Text feature layers\nsentence_features = Embedding(vocab_size, 256, mask_zero=False)(input2)\nmerged = concatenate([img_features_reshaped, sentence_features], axis=1)\nsentence_features = LSTM(256)(merged)\nx = Dropout(0.5)(sentence_features)\nx = add([x, img_features])\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.5)(x)\noutput = Dense(vocab_size, activation='softmax')(x)\n\n# Compile the model\ncaption_model = Model(inputs=[input1, input2], outputs=output)\ncaption_model.compile(loss='categorical_crossentropy', optimizer='adam')\n\n\nModel Visualization\n# Plot model architecture\nplot_model(caption_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n\n# Model summary\ncaption_model.summary()\n\n\nData Generators\nThe following code sets up custom data generators for training and validation. These generators load image-caption pairs from the dataset and prepare batches for model training.\ntrain_generator = CustomDataGenerator(\n    df=train,\n    X_col='image',\n    y_col='caption',\n    batch_size=64,\n    directory=image_path,\n    tokenizer=tokenizer,\n    vocab_size=vocab_size,\n    max_length=max_length,\n    features=features\n)\n\nvalidation_generator = CustomDataGenerator(\n    df=test,\n    X_col='image',\n    y_col='caption',\n    batch_size=64,\n    directory=image_path,\n    tokenizer=tokenizer,\n    vocab_size=vocab_size,\n    max_length=max_length,\n    features=features\n)\nStay tuned for next week where we will finish the model!"
  },
  {
    "objectID": "posts/Week_03/index.html#model-architecture",
    "href": "posts/Week_03/index.html#model-architecture",
    "title": "Week 3",
    "section": "",
    "text": "The model consists of two inputs: one for the image features and another for the tokenized caption. The image features go through a dense layer, and the caption goes through an embedding layer and an LSTM. The outputs are merged, and the model learns to predict the next word in the caption.\nfrom tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add, concatenate, Reshape\n\ninput1 = Input(shape=(1920,))  # Image features from CNN\ninput2 = Input(shape=(max_length,))  # Tokenized caption\n\nimg_features = Dense(256, activation='relu')(input1)\nimg_features_reshaped = Reshape((1, 256))(img_features)\n\nsentence_features = Embedding(vocab_size, 256)(input2)\nmerged = concatenate([img_features_reshaped, sentence_features], axis=1)\nsentence_features = LSTM(256)(merged)\nx = Dropout(0.5)(sentence_features)\nx = add([x, img_features])\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.5)(x)\noutput = Dense(vocab_size, activation='softmax')(x)\n\ncaption_model = Model(inputs=[input1, input2], outputs=output)\ncaption_model.compile(loss='categorical_crossentropy', optimizer='adam')"
  },
  {
    "objectID": "posts/Week_03/index.html#training-the-model",
    "href": "posts/Week_03/index.html#training-the-model",
    "title": "Week 3",
    "section": "",
    "text": "I use the CustomDataGenerator to feed the data into the model, and the model is trained with checkpoints, early stopping, and learning rate reduction to avoid overfitting.\nhistory = caption_model.fit(\n        train_generator,\n        epochs=50,\n        validation_data=validation_generator,\n        callbacks=[checkpoint, earlystopping, learning_rate_reduction]\n)"
  },
  {
    "objectID": "posts/Week_03/index.html#final-thoughts",
    "href": "posts/Week_03/index.html#final-thoughts",
    "title": "Week 3",
    "section": "",
    "text": "Once the model is trained, I can use it to generate captions for unseen images. In the next post, I’ll show how the model performs and how it generates captions for test images.\nStay tuned for more insights into how the model works and its real-world applications!"
  },
  {
    "objectID": "posts/Week_02/index.html",
    "href": "posts/Week_02/index.html",
    "title": "Week 2",
    "section": "",
    "text": "Interesting statistics:\nTotal captions: 40445 Average caption length: 11.78 words Max caption length: 38 words Min caption length: 1 words\n\n\nTokenizing Captions\nThis section handles the preparation of textual data by converting image captions into sequences of integers. It also splits the dataset into training and validation sets, ensuring that 85% of the images are used for training.\n# Tokenize captions\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(captions)\nvocab_size = len(tokenizer.word_index) + 1\nmax_length = max(len(caption.split()) for caption in captions)\n\n# Train-test split\nimages = data['image'].unique().tolist()\nnimages = len(images)\n\nsplit_index = round(0.85 * nimages)\ntrain_images = images[:split_index]\nval_images = images[split_index:]\n\ntrain = data[data['image'].isin(train_images)]\ntest = data[data['image'].isin(val_images)]\n\ntrain.reset_index(inplace=True, drop=True)\ntest.reset_index(inplace=True, drop=True)\n\ntokenizer.texts_to_sequences([captions[1]])[0]\n\n\nImage Feature Extraction\nThis section uses the DenseNet201 model to extract meaningful image features, which are saved in a dictionary for use during model training. Each image is resized, normalized, and converted into a format compatible with the model.\n# Use DenseNet201 for image feature extraction\nmodel = DenseNet201()\nfe = Model(inputs=model.input, outputs=model.layers[-2].output)\n\nimg_size = 224\nfeatures = {}\nfor image in tqdm(data['image'].unique().tolist()):\n    img = load_img(os.path.join(image_path, image), target_size=(img_size, img_size))\n    img = img_to_array(img)\n    img = img / 255.\n    img = np.expand_dims(img, axis=0)\n    feature = fe.predict(img, verbose=0)\n    features[image] = feature\n\n\nCustom Data Generator\nThis custom data generator class facilitates efficient model training by yielding batches of data, including image features and tokenized caption sequences. It handles batch creation, shuffling, and ensures compatibility with the model’s input format.\nclass CustomDataGenerator(Sequence):\n\n    def __init__(self, df, X_col, y_col, batch_size, directory, tokenizer,\n                 vocab_size, max_length, features, shuffle=True):\n\n        self.df = df.copy()\n        self.X_col = X_col\n        self.y_col = y_col\n        self.directory = directory\n        self.batch_size = batch_size\n        self.tokenizer = tokenizer\n        self.vocab_size = vocab_size\n        self.max_length = max_length\n        self.features = features\n        self.shuffle = shuffle\n        self.n = len(self.df)\n\n    def on_epoch_end(self):\n        if self.shuffle:\n            self.df = self.df.sample(frac=1).reset_index(drop=True)\n\n    def __len__(self):\n        return self.n // self.batch_size\n\n    def __getitem__(self, index):\n\n        batch = self.df.iloc[index * self.batch_size:(index + 1) * self.batch_size, :]\n        X1, X2, y = self.__get_data(batch)\n        return (X1, X2), y\n\n    def __get_data(self, batch):\n\n        X1, X2, y = list(), list(), list()\n\n        images = batch[self.X_col].tolist()\n\n        for image in images:\n            feature = self.features[image][0]\n\n            captions = batch.loc[batch[self.X_col] == image, self.y_col].tolist()\n            for caption in captions:\n                seq = self.tokenizer.texts_to_sequences([caption])[0]\n\n                for i in range(1, len(seq)):\n                    in_seq, out_seq = seq[:i], seq[i]\n                    in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]\n                    out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]\n                    X1.append(feature)\n                    X2.append(in_seq)\n                    y.append(out_seq)\n\n        X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n\n        return X1, X2, y\nNext week we will look at the model creation and the image and text feature layers!"
  },
  {
    "objectID": "posts/Week_02/index.html#image-preprocessing",
    "href": "posts/Week_02/index.html#image-preprocessing",
    "title": "Week 2",
    "section": "",
    "text": "For each image, I resize it and normalize the pixel values to a range between 0 and 1:\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n\ndef readImage(path, img_size=224):\n    img = load_img(path, color_mode='rgb', target_size=(img_size, img_size))\n    img = img_to_array(img)\n    img = img / 255.  # Normalize\n    return img\n\ndef text_preprocessing(data):\n    data['caption'] = data['caption'].apply(lambda x: x.lower())\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\", \"\"))\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\", \" \"))\n    data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word) &gt; 1]))\n    data['caption'] = \"startseq \" + data['caption'] + \" endseq\"\n    return data\n\ndata = text_preprocessing(data)\ncaptions = data['caption'].tolist()\ncaptions[:10]\nNext, I will tokenize the captions, splitting them into words and assigning each word a unique integer.\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(captions)\nvocab_size = len(tokenizer.word_index) + 1  # Include padding token\nmax_length = max(len(caption.split()) for caption in captions)\n\n# Split into train and test\nimages = data['image'].unique().tolist()\nsplit_index = round(0.85 * len(images))\ntrain_images = images[:split_index]\nval_images = images[split_index:]\n\ntrain = data[data['image'].isin(train_images)]\ntest = data[data['image'].isin(val_images)]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Senior Project Blog",
    "section": "",
    "text": "Week 3\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 11, 2025\n\n\nScott Townsend\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 4, 2025\n\n\nScott Townsend\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 1\n\n\n\n\n\n\nnews\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 27, 2025\n\n\nScott Townsend\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Spatial Project of the U.S.\n\n\n\n\n\n\nR\n\n\nData Viz\n\n\n\n\n\n\n\n\n\nMar 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nStreaming Services Analysis\n\n\n\n\n\n\nPython\n\n\nDataviz\n\n\n\n\n\n\n\n\n\nMar 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCrime Statistics Analysis\n\n\n\n\n\n\nPython\n\n\nDataviz\n\n\n\n\n\n\n\n\n\nFeb 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nImage Captioning Tool\n\n\n\n\n\n\nPython\n\n\nNeural Networds\n\n\nLSTM\n\n\n\n\n\n\n\n\n\nJan 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nStudent Performance ML\n\n\n\n\n\n\nPython\n\n\nML\n\n\n\n\n\n\n\n\n\nDec 16, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html#projects",
    "href": "projects.html#projects",
    "title": "📚 Projects",
    "section": "",
    "text": "A collection of my personal projects showcasing my work in data analysis, statistical modeling, and machine learning.\n\n\n\nStreaming Services Data Analysis (January 2025 - Present)\nAnalyzed Netflix, Hulu, Disney+, and Amazon content libraries to uncover trends in movie durations, genres, and ratings.\n\nUtilized Polars and Pandas for efficient data processing.\n\nVisualized insights with Seaborn, Matplotlib, and Plotly, exploring genre popularity, rating distributions, and country-wise content production.\n\nBuilt interactive dashboards and statistical summaries to inform data-driven decisions.\n\nImage Captioning Tool for Visually Impaired Users (December 2024 - Present)\nDeveloped a deep learning model to generate descriptive captions for images, aiding visually impaired individuals and automating social media captioning.\n\nImplemented a CNN (VGG16) for image feature extraction and an LSTM/Transformer for caption generation.\n\nVisualized model attention areas using heatmaps and presented insights through data visualizations.\n\nCrime Analysis\nExploring crime trends through data analysis and visualization.\nStudent Performance Analysis\nAnalyzing factors affecting student academic performance.\nStudent Performance Predictive ML Model\nBuilding a predictive model to forecast student success based on historical data.\nPopulation Growth Analysis\nInvestigating population growth trends using statistical methods.\n\n💡 Feel free to explore my projects for detailed analysis and visualization."
  },
  {
    "objectID": "projects.html#my-current-projects",
    "href": "projects.html#my-current-projects",
    "title": "📚 Projects",
    "section": "",
    "text": "A collection of my personal projects showcasing my work in data analysis, statistical modeling, and machine learning.\n\n\n\nStreaming Services Data Analysis (January 2025 - Present)\nAnalyzed Netflix, Hulu, Disney+, and Amazon content libraries to uncover trends in movie durations, genres, and ratings.\n\nUtilized Polars and Pandas for efficient data processing.\n\nVisualized insights with Seaborn, Matplotlib, and Plotly, exploring genre popularity, rating distributions, and country-wise content production.\n\nBuilt interactive dashboards and statistical summaries to inform data-driven decisions.\n\nImage Captioning Tool for Visually Impaired Users (December 2024 - Present)\nDeveloped a deep learning model to generate descriptive captions for images, aiding visually impaired individuals and automating social media captioning.\n\nImplemented a CNN (VGG16) for image feature extraction and an LSTM/Transformer for caption generation.\n\nVisualized model attention areas using heatmaps and presented insights through data visualizations.\n\nCrime Analysis\nExploring crime trends through data analysis and visualization. Developed interactive visualizations in Python (Matplotlib, Plotly) to highlight trends in the demographics of offenders and victims.\n\nApplied data wrangling techniques to identify patterns and trends, offering actionable insights into public safety strategies.\nAnalyzed offender-victim relationships to uncover societal trends influencing crime rates.\n\nStudent Performance Analysis\nAnalyzing factors affecting student academic performance.\nStudent Performance Predictive ML Model\nBuilding a predictive model to forecast student success based on historical data.\nPopulation Growth Analysis\nInvestigating population growth trends using statistical methods.\n\n💡 Feel free to explore my projects for detailed analysis and visualization."
  },
  {
    "objectID": "projects/Crime_Analysis/index.html",
    "href": "projects/Crime_Analysis/index.html",
    "title": "Crime Statistics Analysis",
    "section": "",
    "text": "Crime Count by Location\n\n\nCrime is overwhelmingly concentrated in residences and public streets, with significantly lower rates in other locations. This highlights the need for targeted policing and crime prevention efforts in these high-risk areas. Understanding the underlying factors driving crime in homes and streets is crucial for effective intervention.\n\n\n\n\n\n\nOffender Race Distribution\n\n\nThe data shows a disproportionate number of offenders identified as Black or African American and White, raising questions about socio-economic influences and systemic factors. A large number of “Not Specified” and “Unknown” entries also point to gaps in data collection, emphasizing the need for improved reporting practices to support more accurate analysis and intervention.\n\n\n\n\n\n\nVictim Race Distribution\n\n\nWhite individuals are the most frequently reported victims, followed by Black or African American individuals, though with a substantial gap between the two. The presence of “Unknown” and “Not Specified” categories signals incomplete data, limiting a full understanding of victimization trends. More precise data collection is essential for targeted crime prevention and community support.\n\n\n\n\n\n\nCrime Trends Over Time\n\n\nProperty crimes, particularly vandalism, dominate this crime landscape, forming a “long tail” distribution where a few offenses are highly prevalent while others occur infrequently. While addressing property damage is a priority, law enforcement must also be prepared to tackle a diverse range of lesser-reported crimes. Understanding the factors driving both common and rare offenses is key to effective crime prevention in the city."
  },
  {
    "objectID": "projects/Crime_Analysis/index.html#analysis-overview",
    "href": "projects/Crime_Analysis/index.html#analysis-overview",
    "title": "Criminal Incident Analysis",
    "section": "",
    "text": "The analysis is structured into several key sections, each exploring a different aspect of criminal incidents. Below are the main insights drawn from the data:\n\n\nThis visualization provides an overview of the frequency of criminal incidents based on their location. Understanding where crimes occur helps in targeting both public and private safety initiatives.\n\n\n\n\nResidential areas are the most common settings for criminal incidents, followed closely by public locations such as streets and sidewalks. These findings underscore the need for improved safety measures in both private homes and public spaces.\n\n\n\n\n\nThe following charts show the age distributions of both offenders and victims, offering insights into the typical age groups involved in these incidents.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOffenders: The most common age group among offenders is 20–29, followed by 30–39. This indicates that younger adults are more likely to be involved in criminal activities.\nVictims: Victimization also peaks in the 20–29 age group, suggesting a strong overlap between the ages of offenders and victims. This may reflect social or environmental factors influencing both offending and victimization patterns.\n\n\n\n\n\nThe charts below compare the ethnicities of offenders and victims to reveal potential patterns of racial disparities in criminal incidents.\n\n\n\nEthnicties of the Offenders\n\n\n\n\n\n\n\n\n\n\nThe majority of both offenders and victims are categorized as non-Hispanic and non-Latino. This suggests that ethnic groups outside of the Hispanic/Latino community are more frequently involved in criminal incidents. Further analysis is needed to explore the sociocultural and systemic factors behind these patterns.\n\n\n\n\n\nThis analysis explores the racial distribution of offenders and victims, investigating whether race-related disparities are present.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOffenders: Both White and African American individuals are most commonly represented among offenders, highlighting possible broader societal and economic influences.\nVictims: White individuals appear to be more frequently victimized, suggesting potential interactions between race, geographic location, and social dynamics. This warrants further investigation into how systemic factors contribute to these disparities.\n\n\n\n\n\nThis chart categorizes the types of offenses involved in the reported incidents, giving us a detailed breakdown of criminal activity.\n\n\n\n\n\n\n\n\nProperty destruction is the most prevalent offense, followed by simple assaults, breaking and entering, and drug-related crimes. This breakdown provides a clearer picture of the types of criminal activities most commonly occurring in these incidents.\n\n\n\n\n\nThe following chart provides an overview of the types of weapons used during criminal incidents, offering valuable insights into the role of weaponry in violent crime.\n\n\n\n\n\n\n\n\nPersonal weapons (hands, fists, etc.) are the most frequently used in incidents, followed by firearms and knives. The prominence of physical altercations emphasizes the need for preventive measures around conflict de-escalation, while the prevalence of firearms and knives highlights the importance of effective weapon control.\n\n\n\n\n\nThis chart examines the relationship dynamics between offenders and victims, uncovering patterns in how these relationships might influence criminal behavior.\n\n\n\n\n\n\n\n\nThe most common relationship between offenders and victims is that of strangers, where no prior connection exists between the parties. Other significant relationships include boyfriends/girlfriends, friends, spouses, and acquaintances. This indicates a range of scenarios, from random violence to incidents stemming from interpersonal conflicts."
  },
  {
    "objectID": "projects/Crime_Analysis/index.html#conclusion",
    "href": "projects/Crime_Analysis/index.html#conclusion",
    "title": "Criminal Incident Analysis",
    "section": "",
    "text": "This analysis uncovers several critical patterns in criminal activity related to gun violence. Key findings include:\n\nYounger individuals, particularly in their 20s and 30s, are more likely to be involved both as offenders and victims.\nThe most common offenses involve property destruction, assaults, and drug-related crimes.\nBoth White and African American individuals are most commonly represented among offenders, while White individuals are more frequently victimized.\nPersonal weapons and firearms are the most common weapons used in violent incidents.\n\nThese insights are crucial for understanding the social dynamics behind gun violence. By further exploring these trends and integrating additional datasets, law enforcement agencies and policymakers can make more informed decisions about crime prevention, resource allocation, and public safety initiatives."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Senior Project Blog",
    "section": "",
    "text": "Ridgelines in R\n\n\n\n\n\n\nR\n\n\nDataviz\n\n\n\n\n\n\n\n\n\nFeb 23, 2025\n\n\nScott Townsend\n\n\n\n\n\n\n\n\n\n\n\n\nRidgelines in Python\n\n\n\n\n\n\nPython\n\n\nDataviz\n\n\n\n\n\n\n\n\n\nFeb 23, 2025\n\n\nScott Townsend\n\n\n\n\n\n\n\n\n\n\n\n\nResume\n\n\n\n\n\n\nQuarto\n\n\nLaTeX\n\n\n\n\n\n\n\n\n\nFeb 23, 2025\n\n\nScott Townsend\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial Charts in R\n\n\n\n\n\n\nR\n\n\nDataviz\n\n\n\n\n\n\n\n\n\nFeb 23, 2025\n\n\nScott Townsend\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 11, 2025\n\n\nScott Townsend\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 4, 2025\n\n\nScott Townsend\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 1\n\n\n\n\n\n\nnews\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 27, 2025\n\n\nScott Townsend\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Week_01/index.html#text-preprocessing",
    "href": "posts/Week_01/index.html#text-preprocessing",
    "title": "Week 1",
    "section": "",
    "text": "Finally, I implemented text preprocessing to clean and format the captions:\ndef text_preprocessing(data):\n    data['caption'] = data['caption'].apply(lambda x: x.lower())\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\", \"\"))\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\", \" \"))\n    data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word) &gt; 1]))\n    data['caption'] = \"startseq \" + data['caption'] + \" endseq\"\n    return data\n\ndata = text_preprocessing(data)\ncaptions = data['caption'].tolist()\ncaptions[:10]\n['startseq child in pink dress is climbing up set of stairs in an entry way endseq',\n 'startseq girl going into wooden building endseq',\n 'startseq little girl climbing into wooden playhouse endseq',\n 'startseq little girl climbing the stairs to her playhouse endseq',\n 'startseq little girl in pink dress going into wooden cabin endseq',\n 'startseq black dog and spotted dog are fighting endseq',\n 'startseq black dog and tri-colored dog playing with each other on the road endseq',\n 'startseq black dog and white dog with brown spots are staring at each other in the street endseq',\n 'startseq two dogs of different breeds looking at each other on the road endseq',\n 'startseq two dogs on pavement moving toward each other endseq']\nThis code ensures that captions are converted to lowercase, unwanted characters are removed, and sequences are formatted with start and end tokens. Above we can see a snippet of what this looks like.\n\nWith the data cleaned and preprocessed, I am now ready to move on to building and training the image captioning model. More updates to come!"
  },
  {
    "objectID": "projects/Streaming_Services/index.html",
    "href": "projects/Streaming_Services/index.html",
    "title": "Streaming Services Analysis",
    "section": "",
    "text": "You can slide the charts around to focus on different aspects of it or see different parts."
  },
  {
    "objectID": "projects/Streaming_Services/index.html#analysis-overview",
    "href": "projects/Streaming_Services/index.html#analysis-overview",
    "title": "Streaming Services Analysis",
    "section": "Analysis Overview:",
    "text": "Analysis Overview:\nThis analysis will dive into data from four leading streaming platforms—Netflix, Hulu, Amazon Prime, and Disney+—to explore trends, viewership patterns, and key performance metrics. By examining each service’s unique offerings and subscriber behaviors, we aim to uncover insights that highlight their market positions and user preferences in the ever-evolving digital entertainment landscape.\n\nDistribution of Movie/Show Durations by Streaming Service\n\nimport pandas as pd\nimport polars as pl\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# --- Load Data ---\ndisney = pl.read_csv(\"/Users/scotttow123/Documents/Streaming_Services/Data/disney_plus_titles.csv\")\nhulu = pl.read_csv(\"/Users/scotttow123/Documents/Streaming_Services/Data/hulu_titles.csv\")\nnetflix = pl.read_csv(\"/Users/scotttow123/Documents/Streaming_Services/Data/netflix_titles.csv\")\nprime = pd.read_csv(\"/Users/scotttow123/Documents/Streaming_Services/Data/amazon_prime_titles.csv\")\n\ndisney = disney.with_columns(pl.lit(\"Disney+\").alias(\"platform\"))\nhulu = hulu.with_columns(pl.lit(\"Hulu\").alias(\"platform\"))\nnetflix = netflix.with_columns(pl.lit(\"Netflix\").alias(\"platform\"))\n\ndisney = disney.to_pandas()\nhulu = hulu.to_pandas()\nnetflix = netflix.to_pandas()\n\nprime[\"platform\"] = \"Amazon Prime\"\n\ndata = pd.concat([disney, hulu, netflix, prime], ignore_index=True)\n\ndef convert_duration(duration):\n    if pd.isna(duration):\n        return None\n    duration = str(duration).lower()\n    if \"min\" in duration:\n        return int(duration.replace(\" min\", \"\"))\n    elif \"h\" in duration:\n        parts = duration.split(\" \")\n        hours = int(parts[0].replace(\"h\", \"\")) * 60\n        minutes = int(parts[1].replace(\"min\", \"\")) if len(parts) &gt; 1 else 0\n        return hours + minutes\n    return None\n\ndata[\"duration_minutes\"] = data[\"duration\"].apply(convert_duration)\n\ndata[\"date_added\"] = pd.to_datetime(data[\"date_added\"], errors=\"coerce\")\ndata[\"year_added\"] = data[\"date_added\"].dt.year\n\nplt.figure(figsize=(8, 6))\nsns.violinplot(data=data, x=\"platform\", y=\"duration_minutes\", palette=\"Set2\")\nplt.title(\"Distribution of Movie/Show Durations by Streaming Service\")\nplt.xlabel(\"Streaming Service\")\nplt.ylabel(\"Duration (Minutes)\")\nplt.xticks(rotation=45)\nplt.show()\n\n/var/folders/zs/zycd14f96ks5gpx0bsb97xtr0000gn/T/ipykernel_65863/3666423932.py:43: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\nThe violin plot reveals a striking contrast in content duration across platforms. Amazon Prime stands out with the broadest range, extending to exceptionally lengthy content (up to 600 minutes), suggesting a focus on longer films and series. In contrast, Disney+ exhibits the shortest content range, implying a preference for more concise viewing experiences. Netflix and Hulu fall somewhere in between, with Netflix showing a slightly wider spread than Hulu.\n\n\nNumber of Titles Added Over Time\n\nplt.figure(figsize=(18, 6))\ndf_yearly = data.groupby([\"year_added\", \"platform\"]).size().reset_index(name=\"count\")\nsns.lineplot(data=df_yearly, x=\"year_added\", y=\"count\", hue=\"platform\", marker=\"o\")\nplt.title(\"Number of Titles Added Over Time\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Number of Titles\")\nplt.legend(title=\"Streaming Service\")\nplt.show()\n\n\n\n\n\n\n\n\nThis chart highlights the aggressive content acquisition strategy employed by Netflix. Starting around 2016, Netflix embarked on a dramatic expansion of its library, consistently adding new titles at a pace that far surpassed its competitors. While other platforms also increased their offerings, their growth trajectories were notably less steep, underscoring Netflix’s commitment to content dominance.\n\n\nDistribution of Movie Ratings Across Platforms\n\nplt.figure(figsize=(30, 6))\nsns.histplot(data=data, x=\"rating\", hue=\"platform\", multiple=\"stack\", palette=\"Set1\", shrink=0.8)\nplt.title(\"Distribution of Movie Ratings Across Platforms\")\nplt.xlabel(\"Rating\")\nplt.ylabel(\"Count of Titles\")\nplt.xticks(rotation=45)\nplt.legend(title=\"Streaming Service\")\nplt.show()\n\n/var/folders/zs/zycd14f96ks5gpx0bsb97xtr0000gn/T/ipykernel_65863/2575985119.py:7: UserWarning:\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\n\n\n\n\nThe distribution of movie ratings reveals distinct content preferences across platforms. Amazon Prime caters to a more mature audience with a significant concentration of TV-MA rated content. In contrast, Disney+ likely focuses on family-friendly viewing, as suggested by its lower proportion of mature ratings (though specific data on G and PG ratings is not available here). Netflix and Hulu demonstrate a more balanced approach, offering a mix of ratings to appeal to a broader audience."
  },
  {
    "objectID": "projects/Streaming_Services/index.html#conclusion",
    "href": "projects/Streaming_Services/index.html#conclusion",
    "title": "Streaming Services Analysis",
    "section": "Conclusion:",
    "text": "Conclusion:\nThis analysis has illuminated key differences in content strategies across the four streaming platforms. Each exhibits distinct trends in content offerings, duration, and release timing, catering to diverse viewer preferences. Amazon Prime, with its extensive library and broad range of content durations, including a substantial offering of mature titles, appears to prioritize attracting a diverse and mature audience. Netflix, while showing signs of slowing growth in recent years, still boasts a massive library and a history of aggressive content acquisition, suggesting a focus on breadth and variety."
  },
  {
    "objectID": "posts/Ridgelines/index.html",
    "href": "posts/Ridgelines/index.html",
    "title": "Ridgelines in Python",
    "section": "",
    "text": "In the world of data visualization, understanding the distribution of data across different categories is crucial. Ridgeline plots, also known as joyplots, offer an elegant and effective way to visualize these distributions. This blog post will guide you through creating ridgeline plots in Python using seaborn and matplotlib.\n\n\nRidgeline plots display the distribution of a numerical variable across multiple categories by plotting density estimates (or histograms) that are stacked vertically and slightly overlapped. This creates a “ridgeline” effect, making it easy to compare the distributions of different groups.\nThese plots are particularly useful for:\n\nComparing distributions: Quickly identifying differences in shape, spread, and central tendency across categories.\nIdentifying patterns: Spotting trends and shifts in data that might be obscured in other visualization types.\nEnhancing visual appeal: Creating engaging and informative graphics.\n\n\n\n\nLet’s illustrate how to create ridgeline plots using a simulated dataset of monthly temperature distributions. We’ll utilize the seaborn and matplotlib libraries, which are essential for this task.\nFirst, ensure you have the necessary libraries installed:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(123)\n\nmonths = ['January', 'February', 'March', 'April', 'May', 'June', \n          'July', 'August', 'September', 'October', 'November', 'December']\nn = 100\n\ndata = pd.DataFrame({\n    'month': np.repeat(months, n),\n    'temperature': np.concatenate([\n        np.random.normal(20, 5, n),    # January\n        np.random.normal(25, 6, n),    # February\n        np.random.normal(30, 7, n),    # March\n        np.random.normal(40, 8, n),    # April\n        np.random.normal(50, 9, n),    # May\n        np.random.normal(60, 10, n),   # June\n        np.random.normal(65, 10, n),   # July\n        np.random.normal(62, 9, n),    # August\n        np.random.normal(55, 8, n),    # September\n        np.random.normal(45, 7, n),    # October\n        np.random.normal(35, 6, n),    # November\n        np.random.normal(28, 5, n)     # December\n    ])\n})\n\ndata['month'] = pd.Categorical(data['month'], categories=months, ordered=True)\n\nplt.figure(figsize=(10, 8))\nsns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n\ng = sns.FacetGrid(data, row=\"month\", hue=\"month\", aspect=15, height=0.5, palette=\"plasma\", row_order=months[::-1])  \n\ng.map_dataframe(sns.kdeplot, \"temperature\", fill=True, alpha=1)\n\ndef add_median(data, **kwargs):\n    median = data['temperature'].median()\n    plt.axvline(median, color='black', linestyle='--', linewidth=1)\n\ng.map_dataframe(add_median)\n\ndef label(data, color, label): \n    ax = plt.gca()\n    ax.text(0, 0.2, label, fontweight=\"bold\", color=color, ha=\"left\", va=\"center\", transform=ax.transAxes)\n\ng.map_dataframe(label)\n\ng.set_titles(\"\")\ng.set(yticks=[])\ng.despine(left=True)\ng.fig.subplots_adjust(hspace=-0.25)\ng.set_axis_labels(\"Average temperature (F)\", \"\")\ng.fig.suptitle(\"Monthly Temperature Distribution\", fontsize=16)\n\nplt.show()\nThis python code should create the following chart:"
  },
  {
    "objectID": "projects/Streaming_Services/index.html#content-volume-and-trends",
    "href": "projects/Streaming_Services/index.html#content-volume-and-trends",
    "title": "Streaming Services Analysis",
    "section": "",
    "text": "This visualization illustrates the proportion of the different platforms and their count of releases throughout the years. It appears that Amazon Prime has the highest count of releases for now, with Netflix close behind.\n\n\n\n\nThis line graph tracks the growth in content offerings for each platform over time. We can see here that among the platforms Amazon Prime again has the highest release count through the years. Although, Disney+ seems to have remained steady with their releases over time.\n\n\n\n\nThis visualization highlights the release patterns of movies and TV shows over time across all platforms. This graphic displays how much more prominent TV shows has become over the years with their value increasing more than Movies have."
  },
  {
    "objectID": "projects/Streaming_Services/index.html#content-duration-analysis",
    "href": "projects/Streaming_Services/index.html#content-duration-analysis",
    "title": "Streaming Services Analysis",
    "section": "",
    "text": "This plot provides a comparative analysis of the average content duration on each platform. We have Movie and TV shows being displayed, and it appears that Amazon Prime has the longest duration in both movies and tv shows, while Disney+ has the shortest.\n\n\n\n\nThis is another boxplot that examines the variation in content duration across all platforms, showing again how Amazon Prime dominates in terms of content duration with Netflix coming in second place.\n\n\n\n\nHere is one final boxplot offering a unique point of view displaying the distribution of ratings across the different platforms. It’s interesting seeing the different shapes/sizes of each box plot. Amazon Prime seems to have the widest distribution of ratings compared to the rest.\n\n\n\n\nThis chart explores the relationship between content duration and release year. Something interesting worth noting is how dominant Amazon Prime seems to be in this graphic, which makes sense in terms of our other graphics.\n\n\n\n\nHere is one additional scatter plot, showing us a more unique view of the relase year vs duration, with the rating being colored. The blue and pink seems to stand out the most among all the other colors, indicating that TV-14 and TV-Y are the most popular."
  },
  {
    "objectID": "posts/Resume/index.html#implementation",
    "href": "posts/Resume/index.html#implementation",
    "title": "Resume",
    "section": "Implementation",
    "text": "Implementation\nThe development of this resume is somewhat complex as it implements LaTeX in Quarto to beautifully format the perfect Resume. I have been using this in VS Code, where you can easily render the work once you format or adjust the LaTeX and markdown syntax to your liking.\nAs I have been editing my resume in VS Code using this tool, I have personally found the ability to edit the document while simultaneously viewing the changes side by side to be extremely helpful. Below is a snippet of what it looks like for me as I am editing.\n\nPlease note, for now this is only able to render the LaTeX resume document into a pdf. I plan to mess around and adjust some things in the near future so you can export it as an html if thats the sort of thing that you need as well.\nAdditionally, some cool features of the exported pdf include clickable links (all text in blue are actual links) that the reader can visit, so populate those spaces with urls to your work to make yourself more discoverable.\nThe finished version can be seen below:\n\nYou can fork or clone the repository using the below link if you want to implement this somehow! All credit goes to Alex Bass or acbass49 on GitHub. I have only made minor adjustments and changes to suit my needs. I only want to make this more discoverable by others struggling to find templates or formats that they enjoy or are satisfied with.\nCopy and paste this link into an IDE terminal of your choice (VS Code preferred) and update the files so it is personalized to you.\n# Clone this repository and adjust it to your liking! \ngit clone https://github.com/acbass49/CV_Quarto.git"
  },
  {
    "objectID": "posts/Resume/index.html#introduction",
    "href": "posts/Resume/index.html#introduction",
    "title": "Resume",
    "section": "Introduction",
    "text": "Introduction\nI remember making my first resume when I was in High School about 8 years as a Word Document. I hated how difficult it was to format it to my liking. I struggled with finding good templates and constantly shifted the wording, placement, and overall style of my Resume. Recently, in my Senior Data Science Project course my professor admonished prospective Data Scientists for still using Word, when tools such as Quarto existed. This got me interested in creating a Resume that exported a Mark down document into a pdf. I was directed to several resources, where I ultimately landed on the repository below that you can customized to your liking by: Alex Bass or acbass49 on GitHub"
  },
  {
    "objectID": "posts/Ridgelines2/index.html",
    "href": "posts/Ridgelines2/index.html",
    "title": "Ridgelines in R",
    "section": "",
    "text": "In the realm of data visualization, the quest for insightful and aesthetically pleasing representations is ongoing. Enter ridgeline plots, a powerful tool for displaying distributions across multiple categories. This blog post will explore what ridgeline plots are, how to create them in R using the ggplot2 and ggridges packages, and why they can be a valuable addition to your data analysis toolkit.\n\n\nRidgeline plots, also known as joyplots, are a type of visualization that displays the distribution of a numerical variable for several groups. They achieve this by plotting density estimates (or histograms) for each group, stacked vertically and slightly overlapping. This overlapping effect creates a “ridgeline” appearance, hence the name.\nThese plots are particularly effective for:\n\nComparing distributions: Quickly observe how distributions vary across different categories.\nIdentifying patterns: Spot trends and shifts in data that might be obscured in other visualization types.\nEnhancing visual appeal: Create visually engaging and informative graphics.\n\n\n\n\nTo illustrate how to create ridgeline plots, let’s use a simulated dataset of monthly temperature distributions. We’ll utilize the ggplot2 and ggridges packages, which are essential for this task.\nlibrary(ggplot2)\nlibrary(ggridges)\n\nset.seed(123)\n\nmonths &lt;- month.name \nn &lt;- 100 \n\ndata &lt;- data.frame(\n  month = rep(months, each = n),\n  temperature = c(\n    rnorm(n, mean = 20, sd = 5),    # January\n    rnorm(n, mean = 25, sd = 6),    # February\n    rnorm(n, mean = 30, sd = 7),    # March\n    rnorm(n, mean = 40, sd = 8),    # April\n    rnorm(n, mean = 50, sd = 9),    # May\n    rnorm(n, mean = 60, sd = 10),   # June\n    rnorm(n, mean = 65, sd = 10),   # July\n    rnorm(n, mean = 62, sd = 9),    # August\n    rnorm(n, mean = 55, sd = 8),    # September\n    rnorm(n, mean = 45, sd = 7),    # October\n    rnorm(n, mean = 35, sd = 6),    # November\n    rnorm(n, mean = 28, sd = 5)     # December\n  )\n)\n\ndata$month &lt;- factor(data$month, levels = months)\n\nggplot(data, aes(x = temperature, y = month, fill = month)) +\n  geom_density_ridges(\n    scale = 0.95, \n    rel_min_height = 0.01, \n    quantile_lines = TRUE, \n    quantile_fun = function(x, ...) median(x)\n  ) +\n  scale_fill_viridis_d(option = \"plasma\", direction = -1) + \n  labs(\n    x = \"Average temperature (F)\",\n    y = NULL, \n    title = \"Monthly Temperature Distribution\"\n  ) +\n  theme_ridges(grid = FALSE) + \n  theme(legend.position = \"none\") \nThe above code in R will create the following image:"
  },
  {
    "objectID": "posts/Ridgelines2/index.html#what-are-ridgeline-plots",
    "href": "posts/Ridgelines2/index.html#what-are-ridgeline-plots",
    "title": "Ridgelines in R",
    "section": "",
    "text": "Ridgeline plots, also known as joyplots, are a type of visualization that displays the distribution of a numerical variable for several groups. They achieve this by plotting density estimates (or histograms) for each group, stacked vertically and slightly overlapping. This overlapping effect creates a “ridgeline” appearance, hence the name.\nThese plots are particularly effective for:\n\nComparing distributions: Quickly observe how distributions vary across different categories.\nIdentifying patterns: Spot trends and shifts in data that might be obscured in other visualization types.\nEnhancing visual appeal: Create visually engaging and informative graphics."
  },
  {
    "objectID": "posts/Ridgelines2/index.html#creating-ridgeline-plots-in-r",
    "href": "posts/Ridgelines2/index.html#creating-ridgeline-plots-in-r",
    "title": "Ridgelines in R",
    "section": "",
    "text": "To illustrate how to create ridgeline plots, let’s use a simulated dataset of monthly temperature distributions. We’ll utilize the ggplot2 and ggridges packages, which are essential for this task.\nlibrary(ggplot2)\nlibrary(ggridges)\n\nset.seed(123)\n\nmonths &lt;- month.name \nn &lt;- 100 \n\ndata &lt;- data.frame(\n  month = rep(months, each = n),\n  temperature = c(\n    rnorm(n, mean = 20, sd = 5),    # January\n    rnorm(n, mean = 25, sd = 6),    # February\n    rnorm(n, mean = 30, sd = 7),    # March\n    rnorm(n, mean = 40, sd = 8),    # April\n    rnorm(n, mean = 50, sd = 9),    # May\n    rnorm(n, mean = 60, sd = 10),   # June\n    rnorm(n, mean = 65, sd = 10),   # July\n    rnorm(n, mean = 62, sd = 9),    # August\n    rnorm(n, mean = 55, sd = 8),    # September\n    rnorm(n, mean = 45, sd = 7),    # October\n    rnorm(n, mean = 35, sd = 6),    # November\n    rnorm(n, mean = 28, sd = 5)     # December\n  )\n)\n\ndata$month &lt;- factor(data$month, levels = months)\n\nggplot(data, aes(x = temperature, y = month, fill = month)) +\n  geom_density_ridges(\n    scale = 0.95, \n    rel_min_height = 0.01, \n    quantile_lines = TRUE, \n    quantile_fun = function(x, ...) median(x)\n  ) +\n  scale_fill_viridis_d(option = \"plasma\", direction = -1) + \n  labs(\n    x = \"Average temperature (F)\",\n    y = NULL, \n    title = \"Monthly Temperature Distribution\"\n  ) +\n  theme_ridges(grid = FALSE) + \n  theme(legend.position = \"none\") \nThe above code in R will create the following image:"
  },
  {
    "objectID": "posts/Ridgelines/index.html#what-are-ridgeline-plots",
    "href": "posts/Ridgelines/index.html#what-are-ridgeline-plots",
    "title": "Ridgelines in Python",
    "section": "",
    "text": "Ridgeline plots display the distribution of a numerical variable across multiple categories by plotting density estimates (or histograms) that are stacked vertically and slightly overlapped. This creates a “ridgeline” effect, making it easy to compare the distributions of different groups.\nThese plots are particularly useful for:\n\nComparing distributions: Quickly identifying differences in shape, spread, and central tendency across categories.\nIdentifying patterns: Spotting trends and shifts in data that might be obscured in other visualization types.\nEnhancing visual appeal: Creating engaging and informative graphics."
  },
  {
    "objectID": "posts/Ridgelines/index.html#creating-ridgeline-plots-in-python",
    "href": "posts/Ridgelines/index.html#creating-ridgeline-plots-in-python",
    "title": "Ridgelines in Python",
    "section": "",
    "text": "Let’s illustrate how to create ridgeline plots using a simulated dataset of monthly temperature distributions. We’ll utilize the seaborn and matplotlib libraries, which are essential for this task.\nFirst, ensure you have the necessary libraries installed:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(123)\n\nmonths = ['January', 'February', 'March', 'April', 'May', 'June', \n          'July', 'August', 'September', 'October', 'November', 'December']\nn = 100\n\ndata = pd.DataFrame({\n    'month': np.repeat(months, n),\n    'temperature': np.concatenate([\n        np.random.normal(20, 5, n),    # January\n        np.random.normal(25, 6, n),    # February\n        np.random.normal(30, 7, n),    # March\n        np.random.normal(40, 8, n),    # April\n        np.random.normal(50, 9, n),    # May\n        np.random.normal(60, 10, n),   # June\n        np.random.normal(65, 10, n),   # July\n        np.random.normal(62, 9, n),    # August\n        np.random.normal(55, 8, n),    # September\n        np.random.normal(45, 7, n),    # October\n        np.random.normal(35, 6, n),    # November\n        np.random.normal(28, 5, n)     # December\n    ])\n})\n\ndata['month'] = pd.Categorical(data['month'], categories=months, ordered=True)\n\nplt.figure(figsize=(10, 8))\nsns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n\ng = sns.FacetGrid(data, row=\"month\", hue=\"month\", aspect=15, height=0.5, palette=\"plasma\", row_order=months[::-1])  \n\ng.map_dataframe(sns.kdeplot, \"temperature\", fill=True, alpha=1)\n\ndef add_median(data, **kwargs):\n    median = data['temperature'].median()\n    plt.axvline(median, color='black', linestyle='--', linewidth=1)\n\ng.map_dataframe(add_median)\n\ndef label(data, color, label): \n    ax = plt.gca()\n    ax.text(0, 0.2, label, fontweight=\"bold\", color=color, ha=\"left\", va=\"center\", transform=ax.transAxes)\n\ng.map_dataframe(label)\n\ng.set_titles(\"\")\ng.set(yticks=[])\ng.despine(left=True)\ng.fig.subplots_adjust(hspace=-0.25)\ng.set_axis_labels(\"Average temperature (F)\", \"\")\ng.fig.suptitle(\"Monthly Temperature Distribution\", fontsize=16)\n\nplt.show()\nThis python code should create the following chart:"
  },
  {
    "objectID": "posts/Spatial/index.html",
    "href": "posts/Spatial/index.html",
    "title": "Spatial Charts in R",
    "section": "",
    "text": "Spatial charts are a powerful way to visualize geographic data, allowing for insightful analysis of regional trends, patterns, and distributions. In R, spatial charts can be created using libraries like ggplot2, sf, and USAboundaries, among others.\nHere is one I created in R!\n\n\n\nGeographical Insights: Easily identify trends that vary across regions.\nEnhanced Data Interpretation: Spot high-density areas, hot spots, or outliers.\nInteractive Potential: Use tools like plotly to make maps dynamic and exploratory.\n\n\n\nlibrary(USAboundaries)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(plotly)\n\nwest_virginia_counties &lt;- us_counties() %&gt;%\n  select(geoid, name, state_abbr, geometry) %&gt;%  \n  filter(state_abbr == \"WV\") %&gt;%\n  mutate(population = runif(n(), 5000, 1000000)) %&gt;%  \n  mutate(pop_density = population / as.numeric(st_area(geometry))) \n\nwv_map &lt;- ggplot() +\n  geom_sf(data = virginia_counties, aes(fill = log1p(pop_density), text = name), color = \"white\") +\n  scale_fill_viridis_c(option = \"magma\", name = \"Pop Density (log)\") +\n  labs(\n    title = \"Population Density by County in West Virginia\",\n    subtitle = \"Log-transformed for better visualization\",\n    caption = \"Source: USAboundaries R package\"\n  ) +\n  theme_minimal()\n\nggplotly(wv_map, tooltip = \"text\")"
  },
  {
    "objectID": "posts/Spatial/index.html#why-use-spatial-charts",
    "href": "posts/Spatial/index.html#why-use-spatial-charts",
    "title": "Spatial Charts in R",
    "section": "",
    "text": "Geographical Insights: Easily identify trends that vary across regions.\nEnhanced Data Interpretation: Spot high-density areas, hot spots, or outliers.\nInteractive Potential: Use tools like plotly to make maps dynamic and exploratory."
  },
  {
    "objectID": "posts/Spatial/index.html#how-to-make-them",
    "href": "posts/Spatial/index.html#how-to-make-them",
    "title": "Spatial Charts in R",
    "section": "",
    "text": "library(USAboundaries)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(plotly)\n\nwest_virginia_counties &lt;- us_counties() %&gt;%\n  select(geoid, name, state_abbr, geometry) %&gt;%  \n  filter(state_abbr == \"WV\") %&gt;%\n  mutate(population = runif(n(), 5000, 1000000)) %&gt;%  \n  mutate(pop_density = population / as.numeric(st_area(geometry))) \n\nwv_map &lt;- ggplot() +\n  geom_sf(data = virginia_counties, aes(fill = log1p(pop_density), text = name), color = \"white\") +\n  scale_fill_viridis_c(option = \"magma\", name = \"Pop Density (log)\") +\n  labs(\n    title = \"Population Density by County in West Virginia\",\n    subtitle = \"Log-transformed for better visualization\",\n    caption = \"Source: USAboundaries R package\"\n  ) +\n  theme_minimal()\n\nggplotly(wv_map, tooltip = \"text\")"
  },
  {
    "objectID": "projects/Streaming_Services/index.html#code-used-for-analysis",
    "href": "projects/Streaming_Services/index.html#code-used-for-analysis",
    "title": "Streaming Services Analysis",
    "section": "",
    "text": "import pandas as pd\nimport polars as pl\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# --- Load Data ---\ndisney = pl.read_csv(\"/Users/scotttow123/Documents/Streaming_Services/Data/disney_plus_titles.csv\")\nhulu = pl.read_csv(\"/Users/scotttow123/Documents/Streaming_Services/Data/hulu_titles.csv\")\nnetflix = pl.read_csv(\"/Users/scotttow123/Documents/Streaming_Services/Data/netflix_titles.csv\")\nprime = pd.read_csv(\"/Users/scotttow123/Documents/Streaming_Services/Data/amazon_prime_titles.csv\")\n\ndisney = disney.with_columns(pl.lit(\"Disney+\").alias(\"platform\"))\nhulu = hulu.with_columns(pl.lit(\"Hulu\").alias(\"platform\"))\nnetflix = netflix.with_columns(pl.lit(\"Netflix\").alias(\"platform\"))\n\ndisney = disney.to_pandas()\nhulu = hulu.to_pandas()\nnetflix = netflix.to_pandas()\n\nprime[\"platform\"] = \"Amazon Prime\"\n\ndata = pd.concat([disney, hulu, netflix, prime], ignore_index=True)\n\ndef convert_duration(duration):\n    if pd.isna(duration):\n        return None\n    duration = str(duration).lower()\n    if \"min\" in duration:\n        return int(duration.replace(\" min\", \"\"))\n    elif \"h\" in duration:\n        parts = duration.split(\" \")\n        hours = int(parts[0].replace(\"h\", \"\")) * 60\n        minutes = int(parts[1].replace(\"min\", \"\")) if len(parts) &gt; 1 else 0\n        return hours + minutes\n    return None\n\ndata[\"duration_minutes\"] = data[\"duration\"].apply(convert_duration)\n\ndata[\"date_added\"] = pd.to_datetime(data[\"date_added\"], errors=\"coerce\")\ndata[\"year_added\"] = data[\"date_added\"].dt.year\n\nplt.figure(figsize=(8, 6))\nsns.violinplot(data=data, x=\"platform\", y=\"duration_minutes\", palette=\"Set2\")\nplt.title(\"Distribution of Movie/Show Durations by Streaming Service\")\nplt.xlabel(\"Streaming Service\")\nplt.ylabel(\"Duration (Minutes)\")\nplt.xticks(rotation=45)\nplt.show()\n\nplt.figure(figsize=(18, 6))\ndf_yearly = data.groupby([\"year_added\", \"platform\"]).size().reset_index(name=\"count\")\nsns.lineplot(data=df_yearly, x=\"year_added\", y=\"count\", hue=\"platform\", marker=\"o\")\nplt.title(\"Number of Titles Added Over Time\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Number of Titles\")\nplt.legend(title=\"Streaming Service\")\nplt.show()\n\nplt.figure(figsize=(30, 6))\nsns.histplot(data=data, x=\"rating\", hue=\"platform\", multiple=\"stack\", palette=\"Set1\", shrink=0.8)\nplt.title(\"Distribution of Movie Ratings Across Platforms\")\nplt.xlabel(\"Rating\")\nplt.ylabel(\"Count of Titles\")\nplt.xticks(rotation=45)\nplt.legend(title=\"Streaming Service\")\nplt.show()"
  },
  {
    "objectID": "projects/Student_Performance/index.html",
    "href": "projects/Student_Performance/index.html",
    "title": "Student Performance ML",
    "section": "",
    "text": "The dataset, provided in CSV format, underwent an initial examination for missing values to ensure proper analysis. Categorical features, such as gender, race/ethnicity, and parental education level, were converted to numerical representations using one-hot encoding, improving compatibility with machine learning processing.\nNumerical features, including math, reading, and writing scores, were standardized using StandardScaler to eliminate scale biases and promote balanced model training. Additionally, a new feature, average_score, was introduced through feature engineering by calculating the mean of the three scores. This provided a target variable to enhance predictive modeling.\n\n\n\nTwo predictive models, Linear Regression and Random Forest Regressor, were used to analyze the dataset.\n\nLinear Regression serves as a baseline model, offering simplicity and interpretability.\n\nRandom Forest underwent hyperparameter tuning via GridSearchCV, optimizing parameters such as max_depth and n_estimators to enhance predictive power.\n\nThe dataset was split into training and testing subsets (80/20) to ensure an unbiased evaluation of both models. Cross-validation was applied to the Linear Regression model to improve its generalizability. Meanwhile, the Random Forest model achieved excellent performance with optimized parameters, demonstrating its reliability.\n\n\n\nModel performance was assessed using several metrics:\n\nMean Squared Error (MSE)\nR-squared\nMean Absolute Error (MAE)\nRoot Mean Squared Error (RMSE)\n\nThese evaluations confirmed the strengths of both models:\n\nLinear Regression achieved an R-squared of 0.985, indicating high reliability.\n\nRandom Forest achieved an R-squared of 0.973, validated through learning curves and cross-validation scores.\n\n\n\n\nThe results were presented using simple and clear visuals:\n\nScatter plots of actual vs. predicted values showed how well each model performed.\n\nLinear Regression had points clustered more tightly than Random Forest, indicating better prediction accuracy.\n\n\nHistograms of residuals demonstrated that errors were evenly spread, with no clear patterns or biases.\n\nFeature importance analysis highlighted the most influential factors affecting performance.\n\nLearning curves for the Random Forest model showcased its performance as the dataset size increased.\n\n\n\nBelow is an example of a visualization used to assess model performance:\n\n\n\nActual vs Predicted Chart\n\n\nThe chart above compares the performance of Linear Regression and Random Forest models. Both align well with actual values, but Linear Regression shows slightly tighter clustering, indicating its predictions are closer to actual values compared to Random Forest.\n\n\n\n\n\n\nBoxplot of Distribution by Gender\n\n\nThe box plot shows a higher median math score for males, but also greater variability with more high and low outliers compared to females, who cluster closer to the average. This suggests that males are disproportionately represented at both ends of the math achievement spectrum, raising questions about contributing factors and the need for equitable educational practices.\n\n\n\n\n\n\nHistogram\n\n\nThe histograms reveal that math scores exhibit a more left-skewed distribution, indicating a tendency towards lower scores, while reading and writing scores are more normally distributed, suggesting a wider range of performance centered around the average. Notably, writing scores show a slight right skew, implying a potential ceiling effect or a higher proportion of students achieving higher scores in writing.\n\n\n\n\nBias analysis examined potential differences in math scores between genders to ensure fairness in the model’s predictions.\n\nThe model produced an average score of 63.63 for females and 68.72 for males.\n\nThese findings highlight the importance of addressing potential biases in machine learning models to prevent reinforcement of societal inequalities.\n\nThe analysis confirmed that the model does not introduce biases, as the differences in scores align with the dataset’s inherent distribution.\n\n\n\n\nColab Notebook:\nPersonal Project Link"
  },
  {
    "objectID": "projects/Student_Performance/index.html#i.-data-preprocessing",
    "href": "projects/Student_Performance/index.html#i.-data-preprocessing",
    "title": "Student Performance ML",
    "section": "",
    "text": "The dataset, provided in CSV format, underwent an initial examination for missing values to ensure proper analysis. Categorical features, such as gender, race/ethnicity, and parental education level, were converted to numerical representations using one-hot encoding, improving compatibility with machine learning processing.\nNumerical features, including math, reading, and writing scores, were standardized using StandardScaler to eliminate scale biases and promote balanced model training. Additionally, a new feature, average_score, was introduced through feature engineering by calculating the mean of the three scores. This provided a target variable to enhance predictive modeling."
  },
  {
    "objectID": "projects/Student_Performance/index.html#ii.-selection-training-and-ml-model",
    "href": "projects/Student_Performance/index.html#ii.-selection-training-and-ml-model",
    "title": "Student Performance ML",
    "section": "",
    "text": "Two predictive models, Linear Regression and Random Forest Regressor, were used to analyze the dataset.\n\nLinear Regression serves as a baseline model, offering simplicity and interpretability.\n\nRandom Forest underwent hyperparameter tuning via GridSearchCV, optimizing parameters such as max_depth and n_estimators to enhance predictive power.\n\nThe dataset was split into training and testing subsets (80/20) to ensure an unbiased evaluation of both models. Cross-validation was applied to the Linear Regression model to improve its generalizability. Meanwhile, the Random Forest model achieved excellent performance with optimized parameters, demonstrating its reliability."
  },
  {
    "objectID": "projects/Student_Performance/index.html#iii.-interpretation-of-results",
    "href": "projects/Student_Performance/index.html#iii.-interpretation-of-results",
    "title": "Student Performance ML",
    "section": "",
    "text": "Model performance was assessed using several metrics:\n\nMean Squared Error (MSE)\nR-squared\nMean Absolute Error (MAE)\nRoot Mean Squared Error (RMSE)\n\nThese evaluations confirmed the strengths of both models:\n\nLinear Regression achieved an R-squared of 0.985, indicating high reliability.\n\nRandom Forest achieved an R-squared of 0.973, validated through learning curves and cross-validation scores."
  },
  {
    "objectID": "projects/Student_Performance/index.html#iv.-communication-of-results",
    "href": "projects/Student_Performance/index.html#iv.-communication-of-results",
    "title": "Student Performance ML",
    "section": "",
    "text": "The results were presented using simple and clear visuals:\n\nScatter plots of actual vs. predicted values showed how well each model performed.\n\nLinear Regression had points clustered more tightly than Random Forest, indicating better prediction accuracy.\n\n\nHistograms of residuals demonstrated that errors were evenly spread, with no clear patterns or biases.\n\nFeature importance analysis highlighted the most influential factors affecting performance.\n\nLearning curves for the Random Forest model showcased its performance as the dataset size increased.\n\n\n\nBelow is an example of a visualization used to assess model performance:\n\n\n\nActual vs Predicted Chart\n\n\nThe chart above compares the performance of Linear Regression and Random Forest models. Both align well with actual values, but Linear Regression shows slightly tighter clustering, indicating its predictions are closer to actual values compared to Random Forest.\n\n\n\n\n\n\nBoxplot of Distribution by Gender\n\n\nThe box plot shows a higher median math score for males, but also greater variability with more high and low outliers compared to females, who cluster closer to the average. This suggests that males are disproportionately represented at both ends of the math achievement spectrum, raising questions about contributing factors and the need for equitable educational practices.\n\n\n\n\n\n\nHistogram\n\n\nThe histograms reveal that math scores exhibit a more left-skewed distribution, indicating a tendency towards lower scores, while reading and writing scores are more normally distributed, suggesting a wider range of performance centered around the average. Notably, writing scores show a slight right skew, implying a potential ceiling effect or a higher proportion of students achieving higher scores in writing."
  },
  {
    "objectID": "projects/Student_Performance/index.html#v.-ethical-implications",
    "href": "projects/Student_Performance/index.html#v.-ethical-implications",
    "title": "Student Performance ML",
    "section": "",
    "text": "Bias analysis examined potential differences in math scores between genders to ensure fairness in the model’s predictions.\n\nThe model produced an average score of 63.63 for females and 68.72 for males.\n\nThese findings highlight the importance of addressing potential biases in machine learning models to prevent reinforcement of societal inequalities.\n\nThe analysis confirmed that the model does not introduce biases, as the differences in scores align with the dataset’s inherent distribution."
  },
  {
    "objectID": "projects/Student_Performance/index.html#vi.-python-notebooks",
    "href": "projects/Student_Performance/index.html#vi.-python-notebooks",
    "title": "Student Performance ML",
    "section": "",
    "text": "Colab Notebook:\nPersonal Project Link"
  },
  {
    "objectID": "projects/Image_Caption/index.html",
    "href": "projects/Image_Caption/index.html",
    "title": "Image Captioning Tool",
    "section": "",
    "text": "The rapid advancement of artificial intelligence has unlocked exciting possibilities in the realm of computer vision, enabling machines to “see” and interpret images in ways that were previously unimaginable. This project delves into the fascinating field of image captioning, aiming to develop a sophisticated tool that can automatically generate accurate and descriptive captions for images. This capability has far-reaching implications, from assisting visually impaired individuals in understanding visual content to automating image tagging for massive datasets and even generating creative captions for social media posts.\nThis project leverages the power of deep learning, specifically Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks, to build an image captioning model. The model will be trained on the Flickr8k dataset, a rich collection of images paired with human-written captions, enabling it to learn the complex relationship between visual features and textual descriptions."
  },
  {
    "objectID": "projects/Image_Caption/index.html#introduction",
    "href": "projects/Image_Caption/index.html#introduction",
    "title": "Image Captioning Tool",
    "section": "",
    "text": "The rapid advancement of artificial intelligence has unlocked exciting possibilities in the realm of computer vision, enabling machines to “see” and interpret images in ways that were previously unimaginable. This project delves into the fascinating field of image captioning, aiming to develop a sophisticated tool that can automatically generate accurate and descriptive captions for images. This capability has far-reaching implications, from assisting visually impaired individuals in understanding visual content to automating image tagging for massive datasets and even generating creative captions for social media posts.\nThis project leverages the power of deep learning, specifically Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks, to build an image captioning model. The model will be trained on the Flickr8k dataset, a rich collection of images paired with human-written captions, enabling it to learn the complex relationship between visual features and textual descriptions."
  },
  {
    "objectID": "projects/Image_Caption/index.html#project-methodology",
    "href": "projects/Image_Caption/index.html#project-methodology",
    "title": "Image Captioning Tool",
    "section": "2. Project Methodology",
    "text": "2. Project Methodology\nThe project follows a structured methodology, encompassing several key stages:\n\n2.1 Data Preparation\nThe foundation of any successful machine learning project lies in the quality of the data. Therefore, meticulous data preparation is essential. This involves:\n\nData Cleaning: The Flickr8k dataset is carefully examined to identify and rectify any inconsistencies or errors. This includes removing duplicate entries, handling missing captions, and ensuring the validity of image paths.\nText Preprocessing: The captions are transformed into a format suitable for model training. This includes converting text to lowercase, removing punctuation and special characters, and adding special tokens to mark the beginning and end of each caption. This preprocessing step ensures that the model receives clean and consistent textual input.\n\n\n\n2.2 Feature Extraction\nTo enable the model to “understand” the visual content of images, a pre-trained CNN called DenseNet201 is employed. This powerful CNN has been trained on a massive dataset of images and has learned to extract meaningful features from images, such as edges, textures, and objects. By utilizing DenseNet201, the project benefits from transfer learning, leveraging the knowledge already embedded in the CNN to accelerate the training process and improve performance. The extracted features serve as a rich representation of the image content and are fed as input to the caption generation model.\n\n\n2.3 Caption Generation\nThe heart of the image captioning system lies in the caption generation model. This model ingeniously combines the extracted image features with textual information to generate descriptive captions. The architecture of choice for this task is a Long Short-Term Memory (LSTM) network, a type of recurrent neural network renowned for its ability to process sequential data, such as text. The LSTM network learns to generate captions by predicting the next word in a sequence, given the preceding words and the visual context provided by the image features. This process mimics how humans generate language, building a sentence word by word, taking into account both the preceding context and the visual scene.\n\n\n2.4 Model Training and Evaluation\nThe image captioning model is trained on the meticulously prepared Flickr8k dataset using a supervised learning approach. During training, the model learns to map image features to corresponding captions by minimizing a loss function that quantifies the discrepancy between predicted and actual captions. This iterative process allows the model to fine-tune its parameters and improve its caption generation capabilities.\nOnce trained, the model’s performance is rigorously evaluated using appropriate metrics, such as BLEU score. BLEU (Bilingual Evaluation Understudy) is a widely used metric in natural language processing that assesses the similarity between generated captions and human-written reference captions. By comparing the model’s output to human-generated captions, BLEU provides a quantitative measure of the model’s accuracy and fluency."
  },
  {
    "objectID": "projects/Image_Caption/index.html#example-of-image-caption-pairing",
    "href": "projects/Image_Caption/index.html#example-of-image-caption-pairing",
    "title": "Image Captioning Tool",
    "section": "3. Example of Image-Caption Pairing",
    "text": "3. Example of Image-Caption Pairing\nTo illustrate the nature of the data used in this project, consider the following image and its associated captions:\n\nCaptions:\n\nThere are two blog dogs playing tug-o-war outside.\nA black and brown dog playing with a stick.\nA man with a red helmet is riding on a red bicycle.\nA child doing a handstand on the beach.\nTwo men under a dog hanging by its mouth from a rope.\nA mountain biker is jumping his bike over a rock as another cyclist stands on the trail watching.\nThe woman dressed as a clown is performing outside for others.\nBlack dog running through grass with something orange in its mouth.\nA girl holds up a yellow balloon animal.\nA forest guide points something up to a group of visitors.\nA boy and a girl are riding on a camel in the sand on the beach.\nA child wearing a pink shirt is jumping into the air with their legs and arms spread.\nA hawk diving to catch a small animal.\nTwo dogs running down a path in the woods.\nTwo men grapple up a waterfall.\n\nThis example showcases the diversity of captions and the level of detail that the model is expected to learn."
  },
  {
    "objectID": "projects/Image_Caption/index.html#model-training-visualization",
    "href": "projects/Image_Caption/index.html#model-training-visualization",
    "title": "Image Captioning Tool",
    "section": "4. Model Training Visualization",
    "text": "4. Model Training Visualization\nThe training process of the model can be visualized through loss curves, which show how the model’s performance improves over epochs. Here is a graph that shows the training and validation loss during model training:\n\nThis graph illustrates the decrease in loss over epochs for both the training and validation datasets, indicating that the model is learning effectively."
  },
  {
    "objectID": "projects/Image_Caption/index.html#model-captioning-results",
    "href": "projects/Image_Caption/index.html#model-captioning-results",
    "title": "Image Captioning Tool",
    "section": "5. Model Captioning Results",
    "text": "5. Model Captioning Results\nTo demonstrate the model’s captioning capabilities, here are some generated captions for sample images from the test set:\n\nGenerated Captions:\n\nstartseq football player in red uniform is playing in the field endseq\nstartseq group of people are standing on the street endseq\nstartseq two dogs are running through the grass endseq\nstartseq boy in blue shirt is playing in the water endseq\nstartseq man in black shirt is holding the camera endseq\nstartseq young girl in pink dress and pink dress is blowing bubbles endseq\nstartseq young boy in blue shirt is sitting on the water. endseq\nstartseq man in red shirt is jumping off rock endseq\nstartseq two dogs are running in the grass endseq\nstartseq two dogs are running through the grass endseq\nstartseq young girl in red shirt and white shirt is holding her hair endseq\nstartseq young boy in blue shirt is playing in the water endseq\nstartseq two dogs are playing with the ball endseq\nstartseq two children play in the water endseq\nstartseq two young boys are standing on the beach endseq\n\nThese captions highlight the model’s ability to capture key objects and actions within the images."
  },
  {
    "objectID": "projects/Image_Caption/index.html#project-deliverables",
    "href": "projects/Image_Caption/index.html#project-deliverables",
    "title": "Image Captioning Tool",
    "section": "6. Project Deliverables",
    "text": "6. Project Deliverables\nThe culmination of this project results in several key deliverables:\n\nCleaned Dataset: A meticulously organized and preprocessed version of the Flickr8k dataset, ready for model training and further research.\nTrained Model: A robust and accurate image captioning model capable of generating descriptive captions for a wide range of images.\nPerformance Metrics: A comprehensive evaluation of the model’s performance using relevant metrics, providing insights into its strengths and areas for potential improvement.\nInteractive Demo: A user-friendly interface that allows users to upload images and generate captions in real-time, showcasing the practical application of the developed tool.\nVisualizations: Illustrative visualizations that elucidate the model’s architecture, attention mechanisms, and decision-making process, enhancing understanding and interpretability.\nTechnical Report: A detailed report documenting the project methodology, results, and analysis, serving as a valuable resource for future research and development."
  },
  {
    "objectID": "projects/Image_Caption/index.html#conclusion",
    "href": "projects/Image_Caption/index.html#conclusion",
    "title": "Image Captioning Tool",
    "section": "7. Conclusion",
    "text": "7. Conclusion\nThis project embarks on a journey to develop a practical and valuable image captioning tool by harnessing the power of advanced machine learning techniques. The resulting tool has the potential to empower visually impaired individuals, automate tedious image tagging tasks, and even inspire creative expression in social media. By seamlessly integrating CNNs for image feature extraction and LSTMs for caption generation, this project strives to bridge the gap between visual and textual data, enabling computers to perceive and describe the visual world in a manner akin to humans. The insights gained from this project contribute to the ever-evolving field of artificial intelligence, paving the way for future innovations in image understanding and human-computer interaction."
  },
  {
    "objectID": "projects/Crime_Analysis/index.html#crime-count-by-location",
    "href": "projects/Crime_Analysis/index.html#crime-count-by-location",
    "title": "Crime Statistics Analysis",
    "section": "",
    "text": "Crime Count by Location\n\n\nCrime is overwhelmingly concentrated in residences and public streets, with significantly lower rates in other locations. This highlights the need for targeted policing and crime prevention efforts in these high-risk areas. Understanding the underlying factors driving crime in homes and streets is crucial for effective intervention."
  },
  {
    "objectID": "projects/Crime_Analysis/index.html#offender-race-distribution",
    "href": "projects/Crime_Analysis/index.html#offender-race-distribution",
    "title": "Crime Statistics Analysis",
    "section": "",
    "text": "Offender Race Distribution\n\n\nThe data shows a disproportionate number of offenders identified as Black or African American and White, raising questions about socio-economic influences and systemic factors. A large number of “Not Specified” and “Unknown” entries also point to gaps in data collection, emphasizing the need for improved reporting practices to support more accurate analysis and intervention."
  },
  {
    "objectID": "projects/Crime_Analysis/index.html#victim-race-distribution",
    "href": "projects/Crime_Analysis/index.html#victim-race-distribution",
    "title": "Crime Statistics Analysis",
    "section": "",
    "text": "Victim Race Distribution\n\n\nWhite individuals are the most frequently reported victims, followed by Black or African American individuals, though with a substantial gap between the two. The presence of “Unknown” and “Not Specified” categories signals incomplete data, limiting a full understanding of victimization trends. More precise data collection is essential for targeted crime prevention and community support."
  },
  {
    "objectID": "projects/Crime_Analysis/index.html#crime-trends-over-time",
    "href": "projects/Crime_Analysis/index.html#crime-trends-over-time",
    "title": "Crime Statistics Analysis",
    "section": "",
    "text": "Crime Trends Over Time\n\n\nProperty crimes, particularly vandalism, dominate this crime landscape, forming a “long tail” distribution where a few offenses are highly prevalent while others occur infrequently. While addressing property damage is a priority, law enforcement must also be prepared to tackle a diverse range of lesser-reported crimes. Understanding the factors driving both common and rare offenses is key to effective crime prevention in the city."
  },
  {
    "objectID": "projects/Spatial_View/index.html#analyzing-traffic-accidents-across-the-u.s.",
    "href": "projects/Spatial_View/index.html#analyzing-traffic-accidents-across-the-u.s.",
    "title": "Spatial Project of the U.S.",
    "section": "Analyzing Traffic Accidents Across the U.S.",
    "text": "Analyzing Traffic Accidents Across the U.S.\nIn this project, we delved into the spatial distribution of traffic accidents across the United States. Using a dataset of US accidents, we created an interactive map that visualizes the accident count for each state. The map employs a log10 scale to better represent the wide range of accident counts, from thousands to millions.\n\nKey Analytical Insight\nThe map clearly illustrates the significant variation in traffic accident counts across different states. Larger, more populous states, particularly those along the coasts and in the Southeast, tend to exhibit higher accident counts. This is likely due to a combination of factors, including higher population density, increased traffic volume, and potentially longer average commute distances.\nConversely, states in the Midwest and Mountain regions generally show lower accident counts. This could be attributed to lower population density, less traffic congestion, and potentially different driving habits. However, it’s important to note that even with lower overall counts, the rate of accidents per capita might tell a different story.\nThe use of a log10 scale is crucial here. Without it, the vast differences in accident counts would make it difficult to discern patterns in states with lower numbers. The interactive nature of the plot, achieved through plotly, allows users to hover over each state and view the exact accident count, enhancing the exploratory analysis.\nThere is more work to come! I plan on expanding on this analysis to display the counties for each state and display a heatmap of the accidets across the country.\n\n\nR Code Used\nlibrary(USAboundaries)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(viridis)\nlibrary(data.table)\nlibrary(scales)\nlibrary(ggrepel)\nlibrary(plotly)\n\naccidents &lt;- fread(\"US_Accidents.csv\", select = c(\"State\", \"Severity\", \"Start_Lat\", \"Start_Lon\"))\nstates &lt;- us_states()\naccidents &lt;- accidents %&gt;% mutate(State = toupper(State))\n\naccident_counts &lt;- accidents %&gt;%\n  group_by(State) %&gt;%\n  summarise(accident_count = n(), .groups = \"drop\")\n\nmap_data &lt;- states %&gt;%\n  left_join(accident_counts, by = c(\"state_abbr\" = \"State\"))\n\nmap_data &lt;- map_data %&gt;%\n  filter(!state_abbr %in% c(\"HI\", \"AK\"))\n\nmap_data &lt;- map_data %&gt;%\n  mutate(centroid = st_centroid(geometry),\n         centroid_x = st_coordinates(centroid)[, 1],\n         centroid_y = st_coordinates(centroid)[, 2])\n\nbase_map &lt;- ggplot(map_data) +\n  geom_sf(aes(fill = accident_count,\n              text = paste(name, \"&lt;br&gt;Accident Count:\", accident_count)),\n          color = \"gray20\", size = 0.2) +\n  scale_fill_viridis_c(\n    option = \"inferno\",\n    na.value = \"gray90\",\n    trans = \"log10\",\n    breaks = trans_breaks(\"log10\", function(x) 10^x),\n    labels = comma_format(accuracy = 1)\n  ) +\n  theme_void() +\n  labs(\n    title = \"Traffic Accidents by State in the US\",\n    fill = \"Accident Count\",\n    caption = \"Data Source: [Your Data Source], [Date of Data]\"\n  ) +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    legend.title = element_text(size = 10, face = \"bold\", margin = margin(b = 5)),\n    legend.text = element_text(size = 9),\n    legend.key.size = unit(0.7, \"cm\"),\n    plot.caption = element_text(hjust = 0, size = 10, color = \"gray50\", margin = margin(t = 10)),\n    panel.background = element_rect(fill = \"gray98\", color = NA)\n  ) +\n  coord_sf(crs = st_crs(2163), xlim = c(-2500000, 2500000), ylim = c(-2300000, 730000)) +\n  guides(fill = guide_colorbar(barheight = 10, barwidth = 1))\n\nlabel_layer &lt;- geom_text_repel(aes(x = centroid_x, y = centroid_y, label = state_abbr),\n                               size = 3,\n                               color = \"gray30\",\n                               box.padding = unit(0.5, \"lines\"),\n                               point.padding = unit(0.3, \"lines\"),\n                               segment.color = \"gray50\", segment.size = 0.3,\n                               min.segment.length = 0,\n                               max.overlaps = 20)\n\nfinal_plot &lt;- base_map + label_layer\n\nplotly_plot &lt;- ggplotly(final_plot, tooltip = \"text\")\n\nplotly_plot"
  }
]